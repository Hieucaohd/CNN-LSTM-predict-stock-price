{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255, x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.uint8, name=None))>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMode(keras.Model):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv1 = keras.layers.Conv2D(32, 3, activation='relu')\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.d1 = keras.layers.Dense(128, activation='relu')\n",
    "        self.d2 = keras.layers.Dense(10)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyMode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_metric = keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy_metric = keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_metric = keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy_metric = keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_function(labels, predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss_metric(loss)\n",
    "    train_accuracy_metric(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_function(labels, predictions)\n",
    "    \n",
    "    test_loss_metric(t_loss)\n",
    "    test_accuracy_metric(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.14, Accuracy: 95.87, Test loss: 0.07, Test Accuracy: 97.67\n",
      "Epoch 2, Loss: 0.04, Accuracy: 98.69, Test loss: 0.05, Test Accuracy: 98.23\n",
      "Epoch 3, Loss: 0.02, Accuracy: 99.30, Test loss: 0.05, Test Accuracy: 98.40\n",
      "Epoch 4, Loss: 0.01, Accuracy: 99.59, Test loss: 0.06, Test Accuracy: 98.43\n",
      "Epoch 5, Loss: 0.01, Accuracy: 99.63, Test loss: 0.06, Test Accuracy: 98.34\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss_metric.reset_state()\n",
    "    train_accuracy_metric.reset_state()\n",
    "    test_loss_metric.reset_state()\n",
    "    test_accuracy_metric.reset_state()\n",
    "    \n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "    \n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "        \n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f\"Loss: {train_loss_metric.result():0.2f}, \"\n",
    "        f'Accuracy: {train_accuracy_metric.result() * 100:0.2f}, '\n",
    "        f\"Test loss: {test_loss_metric.result():0.2f}, \"\n",
    "        f\"Test Accuracy: {test_accuracy_metric.result() * 100:0.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras Subclassing API** là một cách tiếp cận để xây dựng mô hình trong Keras bằng cách kế thừa từ lớp `tf.keras.Model`. Phương pháp này cho phép bạn tạo ra các mô hình phức tạp hơn và linh hoạt hơn, đặc biệt khi cần tùy chỉnh cách thức forward pass, loss function hoặc cách thức huấn luyện.\n",
    "\n",
    "### Lợi ích của Keras Subclassing API\n",
    "1. **Tùy chỉnh linh hoạt**: Bạn có thể định nghĩa các lớp riêng, cho phép bạn tự do trong việc xây dựng kiến trúc mạng và cách thức xử lý dữ liệu.\n",
    "2. **Kiểm soát tốt hơn**: Bạn có thể dễ dàng kiểm soát từng bước trong quá trình huấn luyện, từ việc tính toán loss đến cập nhật weights.\n",
    "3. **Khả năng mở rộng**: Thích hợp cho các ứng dụng phức tạp, chẳng hạn như các mô hình có nhiều đầu ra hoặc nhiều loại loss khác nhau.\n",
    "\n",
    "### Ví dụ sử dụng Keras Subclassing API\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        return self.dense2(x)\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "# Định nghĩa optimizer và loss\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Custom training loop\n",
    "for epoch in range(epochs):\n",
    "    for x_batch, y_batch in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch)\n",
    "            loss_value = loss_fn(y_batch, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "```\n",
    "\n",
    "### Tại sao nó có thể hiệu quả hơn so với `model.fit()`?\n",
    "- **Tùy chỉnh quá trình huấn luyện**: Với custom training loop, bạn có thể thêm các logic đặc biệt như điều chỉnh learning rate trong quá trình huấn luyện, áp dụng các hình thức regularization khác nhau, hoặc thực hiện việc kiểm tra điều kiện dừng (early stopping) mà không bị giới hạn bởi các phương thức tự động hóa của `model.fit()`.\n",
    "- **Kiểm soát trực tiếp quá trình tính toán gradient**: Bạn có thể can thiệp vào cách thức gradients được tính toán và áp dụng, giúp tối ưu hóa quá trình huấn luyện cho các mô hình phức tạp hơn.\n",
    "\n",
    "Tóm lại, Keras Subclassing API cho phép bạn xây dựng các mô hình phức tạp và tùy chỉnh cách huấn luyện, mang lại nhiều lợi ích cho các ứng dụng yêu cầu cao về hiệu suất và khả năng mở rộng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.GradientTape()` là một lớp trong TensorFlow được sử dụng để ghi lại các phép toán mà bạn thực hiện trên các tensors để tính toán gradients (đạo hàm) một cách tự động. Nó rất hữu ích trong quá trình huấn luyện mô hình, cho phép bạn dễ dàng tính toán gradient của một hàm mất mát (loss function) đối với các tham số (weights) của mô hình.\n",
    "\n",
    "### Cách hoạt động của `tf.GradientTape()`\n",
    "1. **Ghi lại phép toán**: Khi bạn sử dụng `tf.GradientTape()`, tất cả các phép toán mà bạn thực hiện trên tensors sẽ được ghi lại trong một ngữ cảnh. Điều này cho phép TensorFlow biết cách để tính toán gradient sau đó.\n",
    "   \n",
    "2. **Tính toán gradient**: Sau khi bạn đã thực hiện các phép toán và ghi lại chúng, bạn có thể gọi phương thức `gradient()` để tính toán gradient của một giá trị (như loss) theo một hoặc nhiều tensors.\n",
    "\n",
    "### Cú pháp sử dụng\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Giả sử bạn có một mô hình và một hàm mất mát\n",
    "model = ...  # Khởi tạo mô hình\n",
    "loss_fn = ...  # Hàm mất mát\n",
    "\n",
    "# Khởi tạo GradientTape\n",
    "with tf.GradientTape() as tape:\n",
    "    predictions = model(inputs)  # Tính toán dự đoán\n",
    "    loss_value = loss_fn(targets, predictions)  # Tính toán giá trị mất mát\n",
    "\n",
    "# Tính toán gradients\n",
    "grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "# Cập nhật weights\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "```\n",
    "\n",
    "### Lợi ích của việc sử dụng `tf.GradientTape()`\n",
    "- **Tính toán tự động**: Giúp bạn không phải tính toán gradient thủ công, giảm thiểu lỗi và tiết kiệm thời gian.\n",
    "- **Tùy chỉnh**: Bạn có thể dễ dàng điều chỉnh cách mà gradients được tính toán, cho phép bạn áp dụng các kỹ thuật tối ưu hóa phức tạp hơn.\n",
    "- **Hỗ trợ cho các mô hình phức tạp**: Làm cho việc xây dựng và huấn luyện các mô hình sâu và phức tạp trở nên dễ dàng hơn.\n",
    "\n",
    "### Kết luận\n",
    "`tf.GradientTape()` là một công cụ mạnh mẽ giúp bạn quản lý và tính toán gradients trong TensorFlow, đặc biệt hữu ích trong các quy trình huấn luyện mô hình machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lý do dòng lệnh `grads = tape.gradient(loss_value, model.trainable_variables)` được thực hiện ngoài `with tf.GradientTape() as tape` là vì `tf.GradientTape` chỉ có nhiệm vụ ghi lại các phép toán để tính toán gradient, nhưng không thực hiện việc tính gradient ngay lập tức. Việc tính toán gradient chỉ xảy ra khi bạn gọi `tape.gradient(...)` sau khi các phép toán đã được ghi lại.\n",
    "\n",
    "### Chi tiết\n",
    "1. **Ghi lại các phép toán**: Trong khối `with tf.GradientTape() as tape`, TensorFlow sẽ ghi lại các phép toán được thực hiện trên tensors. Khi bạn tính toán `predictions` và `loss_value`, các phép toán này đều được ghi lại bởi `tape`.\n",
    "\n",
    "2. **Tính gradient sau khi đã có phép toán**: Sau khi ra khỏi khối `with`, các phép toán đã được ghi lại trong `tape`. Chỉ khi gọi `tape.gradient(loss_value, model.trainable_variables)`, TensorFlow mới sử dụng những phép toán đã ghi lại này để tính gradient của `loss_value` với các biến huấn luyện (`trainable_variables`) của mô hình.\n",
    "\n",
    "### Tại sao không tính gradient bên trong `with`?\n",
    "Nếu bạn tính gradient bên trong `with tf.GradientTape()`, thì `tape` sẽ tiếp tục ghi lại các phép toán, điều này không cần thiết và có thể làm tăng sử dụng bộ nhớ. Thay vào đó, việc tính gradient thường được thực hiện ngay sau khi khối `with` kết thúc.\n",
    "\n",
    "### Tóm lại\n",
    "Việc thực hiện `tape.gradient` ngoài `with` giúp cho việc ghi và tính gradient tách biệt, tối ưu bộ nhớ và hiệu năng khi huấn luyện mô hình."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
