{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.layers as layers\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lệnh np.set_printoptions(precision=3, suppress=True) trong numpy được dùng để tùy chỉnh cách hiển thị các mảng numpy trên màn hình. Cụ thể:\n",
    "\n",
    "- precision=3: Xác định số lượng chữ số thập phân hiển thị cho các số dấu phẩy động (floating-point). Ở đây, precision=3 có nghĩa là chỉ hiển thị 3 chữ số thập phân.\n",
    "\n",
    "- suppress=True: Chỉ định việc không hiển thị số mũ khoa học đối với các số rất nhỏ. Nếu suppress=True, những số rất nhỏ (nhưng không phải là 0) sẽ được hiển thị ở dạng thập phân thông thường (ví dụ: 0.001 thay vì 1.000e-03).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "array = np.array([0.00012345, 123.456789, 0.9999])\n",
    "print(array)\n",
    "```\n",
    "\n",
    "Kết quả sẽ là:\n",
    "```python\n",
    "[ 0.     123.457   1.    ]\n",
    "```\n",
    "\n",
    "Trong ví dụ trên:\n",
    "\n",
    "- Các số thập phân được làm tròn đến 3 chữ số thập phân.\n",
    "- Các giá trị nhỏ như 0.00012345 hiển thị dưới dạng 0. thay vì 1.234e-04.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.435</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.1355</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0965</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.585</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.3545</td>\n",
       "      <td>0.2075</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.655</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.092</td>\n",
       "      <td>0.3960</td>\n",
       "      <td>0.2825</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.545</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.1495</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.545</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.3740</td>\n",
       "      <td>0.1695</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
       "0   0.435     0.335   0.110         0.334          0.1355          0.0775   \n",
       "1   0.585     0.450   0.125         0.874          0.3545          0.2075   \n",
       "2   0.655     0.510   0.160         1.092          0.3960          0.2825   \n",
       "3   0.545     0.425   0.125         0.768          0.2940          0.1495   \n",
       "4   0.545     0.420   0.130         0.879          0.3740          0.1695   \n",
       "\n",
       "   Shell weight  Age  \n",
       "0        0.0965    7  \n",
       "1        0.2250    6  \n",
       "2        0.3700   14  \n",
       "3        0.2600   16  \n",
       "4        0.2300   13  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_train = pd.read_csv(\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\",\n",
    "    names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "           \"Viscera weight\", \"Shell weight\", \"Age\"])\n",
    "\n",
    "abalone_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_features = abalone_train.copy()\n",
    "abalone_labels = abalone_features.pop(\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight',\n",
       "       'Viscera weight', 'Shell weight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        7\n",
       "1        6\n",
       "2       14\n",
       "3       16\n",
       "4       13\n",
       "        ..\n",
       "3315    15\n",
       "3316    10\n",
       "3317    11\n",
       "3318    16\n",
       "3319    19\n",
       "Name: Age, Length: 3320, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(abalone_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_features = np.array(abalone_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.435, 0.335, 0.11 , ..., 0.136, 0.077, 0.097],\n",
       "       [0.585, 0.45 , 0.125, ..., 0.354, 0.207, 0.225],\n",
       "       [0.655, 0.51 , 0.16 , ..., 0.396, 0.282, 0.37 ],\n",
       "       ...,\n",
       "       [0.53 , 0.42 , 0.13 , ..., 0.374, 0.167, 0.249],\n",
       "       [0.395, 0.315, 0.105, ..., 0.118, 0.091, 0.119],\n",
       "       [0.45 , 0.355, 0.12 , ..., 0.115, 0.067, 0.16 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3320, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.570</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.180</td>\n",
       "      <td>1.2950</td>\n",
       "      <td>0.3390</td>\n",
       "      <td>0.2225</td>\n",
       "      <td>0.440</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.185</td>\n",
       "      <td>1.3485</td>\n",
       "      <td>0.4930</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.490</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.440</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.3650</td>\n",
       "      <td>0.1655</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>0.110</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.620</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.1295</td>\n",
       "      <td>0.4630</td>\n",
       "      <td>0.2685</td>\n",
       "      <td>0.330</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.505</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6570</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>0.195</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
       "0   0.570     0.465   0.180        1.2950          0.3390          0.2225   \n",
       "1   0.660     0.530   0.185        1.3485          0.4930          0.2450   \n",
       "2   0.440     0.345   0.120        0.3650          0.1655          0.0830   \n",
       "3   0.620     0.475   0.160        1.1295          0.4630          0.2685   \n",
       "4   0.505     0.410   0.135        0.6570          0.2910          0.1330   \n",
       "\n",
       "   Shell weight  Age  \n",
       "0         0.440   12  \n",
       "1         0.490   12  \n",
       "2         0.110    7  \n",
       "3         0.330   10  \n",
       "4         0.195   15  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_test = pd.read_csv(\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/abalone_test.csv\",\n",
    "    names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "           \"Viscera weight\", \"Shell weight\", \"Age\"])\n",
    "\n",
    "abalone_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_features_test = abalone_test.copy()\n",
    "abalone_labels_test = abalone_features_test.pop(\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_features_test = np.array(abalone_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57 , 0.465, 0.18 , ..., 0.339, 0.223, 0.44 ],\n",
       "       [0.66 , 0.53 , 0.185, ..., 0.493, 0.245, 0.49 ],\n",
       "       [0.44 , 0.345, 0.12 , ..., 0.166, 0.083, 0.11 ],\n",
       "       ...,\n",
       "       [0.61 , 0.48 , 0.165, ..., 0.421, 0.264, 0.335],\n",
       "       [0.565, 0.4  , 0.13 , ..., 0.307, 0.167, 0.18 ],\n",
       "       [0.59 , 0.49 , 0.135, ..., 0.422, 0.225, 0.285]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      12\n",
       "1      12\n",
       "2       7\n",
       "3      10\n",
       "4      15\n",
       "       ..\n",
       "845     9\n",
       "846     8\n",
       "847    13\n",
       "848     8\n",
       "849    11\n",
       "Name: Age, Length: 850, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_model = keras.models.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_model.compile(\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 745us/step - accuracy: 0.0000e+00 - loss: 96.4602\n",
      "Epoch 2/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - accuracy: 2.0213e-04 - loss: 35.5010\n",
      "Epoch 3/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - accuracy: 3.1593e-04 - loss: 9.5746\n",
      "Epoch 4/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - accuracy: 6.3478e-04 - loss: 8.2596\n",
      "Epoch 5/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - accuracy: 3.2419e-04 - loss: 8.3060\n",
      "Epoch 6/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - accuracy: 4.1621e-05 - loss: 7.1566\n",
      "Epoch 7/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - accuracy: 3.2812e-05 - loss: 7.1054\n",
      "Epoch 8/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - accuracy: 3.1038e-05 - loss: 6.4620\n",
      "Epoch 9/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - accuracy: 3.1798e-04 - loss: 6.4700\n",
      "Epoch 10/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - accuracy: 1.3616e-04 - loss: 5.9435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23ba0095d00>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_model.fit(abalone_features, abalone_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi sử dụng `keras.Sequential`, mô hình `abalone_model` có thể tự động xác định kích thước đầu vào dựa trên hình dạng của dữ liệu đầu vào (`abalone_features.shape`).\n",
    "\n",
    "Trong trường hợp này, khi bạn huấn luyện mô hình với `abalone_features`, Keras sẽ kiểm tra hình dạng của dữ liệu. Nếu `abalone_features` có dạng `(số mẫu, 7)`, Keras hiểu rằng mỗi mẫu dữ liệu đầu vào có **7 đặc trưng** (features).\n",
    "\n",
    "Điều này được thực hiện nhờ tính năng tự động suy luận hình dạng đầu vào (input shape inference) của Keras. Khi bạn không chỉ rõ `input_shape` cho lớp đầu tiên của mô hình, Keras sẽ nhận diện hình dạng đầu vào từ lô dữ liệu đầu tiên khi mô hình huấn luyện hoặc dự đoán, giúp xác định rằng mỗi mẫu có 7 đặc trưng.\n",
    "\n",
    "### Cách thức hoạt động\n",
    "- Khi bạn gọi `model.fit(...)` hoặc `model.predict(...)` lần đầu tiên, Keras sẽ xác định rằng `abalone_features` có hình dạng `(batch_size, 7)`.\n",
    "- Keras hiểu rằng lớp `Dense(64)` sẽ nhận đầu vào với kích thước 7 (tương ứng với số đặc trưng), và tự động cấu hình mô hình để nhận đúng số đầu vào này.\n",
    "\n",
    "### Nếu muốn chỉ định rõ ràng\n",
    "Bạn cũng có thể chỉ rõ kích thước đầu vào bằng cách cung cấp `input_shape=(7,)` cho lớp đầu tiên, như sau:\n",
    "\n",
    "```python\n",
    "abalone_model = keras.models.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(7,)),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "Cách này giúp đảm bảo rằng mô hình luôn yêu cầu đầu vào với 7 đặc trưng ngay từ đầu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`input_shape=(7,)` là một tham số chỉ rõ kích thước đầu vào của dữ liệu cho lớp đầu tiên của mô hình trong Keras. Cụ thể:\n",
    "\n",
    "- `input_shape=(7,)` có nghĩa là mỗi mẫu đầu vào có **7 đặc trưng (features)**. Dấu phẩy sau số 7 (`(7,)`) cho biết rằng đây là một tuple một chiều (7,) - tức là một vector với 7 giá trị. Đây là dạng thường gặp khi đầu vào là dữ liệu tabular hoặc các mẫu vector trong machine learning.\n",
    "\n",
    "### Ý nghĩa của `input_shape=(7,)`\n",
    "- Nó giúp lớp đầu tiên của mô hình Keras biết rằng mỗi mẫu đầu vào sẽ có đúng 7 đặc trưng. Điều này rất quan trọng để đảm bảo rằng các phép tính toán trên lớp đầu tiên sẽ khớp với hình dạng của dữ liệu.\n",
    "- Nếu `input_shape` không được chỉ định, Keras sẽ đợi dữ liệu thực tế được đưa vào mô hình để tự suy ra hình dạng của đầu vào.\n",
    "\n",
    "### Giá trị khác cho `input_shape`\n",
    "`input_shape` có thể nhận nhiều giá trị khác nhau tùy vào loại dữ liệu:\n",
    "\n",
    "1. **Vector (1D)**: `(n,)`, như `(7,)`, chỉ ra rằng dữ liệu là một vector n phần tử.\n",
    "2. **Ma trận (2D)**: `(height, width)`, như `(28, 28)` cho ảnh grayscale kích thước 28x28.\n",
    "3. **Tensor (3D)**: `(height, width, channels)`, như `(32, 32, 3)` cho ảnh màu 32x32 với 3 kênh (RGB).\n",
    "4. **Tensor nhiều chiều hơn**: Với dữ liệu phức tạp hơn, bạn có thể có `input_shape` với nhiều hơn 3 chiều, chẳng hạn `(time_steps, height, width, channels)` cho dữ liệu video (với các ảnh liên tục theo thời gian).\n",
    "\n",
    "Ví dụ:\n",
    "- **Ảnh grayscale 28x28**: `input_shape=(28, 28, 1)`\n",
    "- **Ảnh màu RGB 32x32**: `input_shape=(32, 32, 3)`\n",
    "- **Chuỗi thời gian 10 bước, mỗi bước có 3 đặc trưng**: `input_shape=(10, 3)`\n",
    "\n",
    "### Ví dụ sử dụng\n",
    "```python\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(7,)),  # Đầu vào là vector 7 phần tử\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "Trong ví dụ này, `input_shape=(7,)` chỉ ra rằng mỗi mẫu có 7 đặc trưng, phù hợp cho các bài toán hồi quy hay phân loại với đầu vào vector.\n",
    "\n",
    "Nếu sau đó chúng ta chuyền vào model một input với shape là (1, 8) (tức có 1 data point và data point có 8 feature) thì sẽ gây ra lỗi\n",
    "```python\n",
    "input = np.array([[0.435, 0.335, 0.11 , 0.334, 0.136, 0.077, 6, 6]])\n",
    "abalone_model.predict(input)\n",
    "```\n",
    "\n",
    "Lỗi là:\n",
    "```python\n",
    "Input 0 of layer \"dense_6\" is incompatible with the layer: expected axis -1 of input shape to have value 7, but received input with shape (1, 8)\n",
    "\n",
    "Arguments received by Sequential.call():\n",
    "  • inputs=tf.Tensor(shape=(1, 8), dtype=float32)\n",
    "  • training=False\n",
    "  • mask=None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 - 0s - 3ms/step - accuracy: 0.0000e+00 - loss: 6.7513\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = abalone_model.evaluate(abalone_features_test, abalone_labels_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.751287460327148"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = keras.layers.Normalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize.adapt(abalone_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`normalize = layers.Normalization()` là một lớp (layer) normalization trong Keras, dùng để chuẩn hóa dữ liệu đầu vào của mô hình. Lớp này nằm trong `tf.keras.layers` và có nhiệm vụ chuẩn hóa dữ liệu sao cho mỗi đặc trưng có **trung bình bằng 0** và **độ lệch chuẩn bằng 1**. Chuẩn hóa giúp các giá trị dữ liệu nằm trong khoảng nhỏ hơn, giúp mô hình hội tụ nhanh hơn và ổn định hơn trong quá trình huấn luyện.\n",
    "\n",
    "### Cách hoạt động của `layers.Normalization`\n",
    "- Khi sử dụng lớp này, bạn cần **tính toán trước** trung bình và độ lệch chuẩn của dữ liệu huấn luyện, sau đó áp dụng chuẩn hóa cho từng đặc trưng (feature).\n",
    "- Sau khi được `adapt()` với dữ liệu huấn luyện, lớp này sẽ chuẩn hóa các giá trị đầu vào dựa trên các giá trị trung bình và độ lệch chuẩn đã tính.\n",
    "\n",
    "### Các bước để sử dụng\n",
    "1. **Khởi tạo lớp**: `normalize = layers.Normalization()`\n",
    "2. **Cập nhật thống kê** bằng cách gọi `normalize.adapt(data)`, trong đó `data` là dữ liệu mẫu.\n",
    "3. **Thêm vào mô hình**: Lớp này sẽ được đặt ở đầu mô hình để chuẩn hóa dữ liệu ngay trước khi vào lớp đầu tiên.\n",
    "\n",
    "### Ví dụ\n",
    "```python\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Khởi tạo lớp Normalization\n",
    "normalize = layers.Normalization()\n",
    "\n",
    "# Tính toán trung bình và độ lệch chuẩn của dữ liệu huấn luyện\n",
    "normalize.adapt(training_data)\n",
    "\n",
    "# Xây dựng mô hình với lớp chuẩn hóa\n",
    "model = keras.models.Sequential([\n",
    "    normalize,  # Áp dụng chuẩn hóa đầu vào\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "### Các tham số của `layers.Normalization`\n",
    "Khi khởi tạo `Normalization`, có một số tham số tùy chọn:\n",
    "- `mean`: Giá trị trung bình tùy chỉnh để chuẩn hóa, nếu không lớp sẽ tự động tính từ dữ liệu.\n",
    "- `variance`: Độ lệch chuẩn tùy chỉnh để chuẩn hóa.\n",
    "\n",
    "Khi không cung cấp `mean` và `variance`, lớp sẽ tự động tính các giá trị này bằng cách gọi `adapt()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lớp `Normalization` sẽ chuẩn hóa dữ liệu đầu vào bằng cách áp dụng công thức:\n",
    "\n",
    "\\[\n",
    "\\text{normalized\\_value} = \\frac{\\text{value} - \\text{mean}}{\\sqrt{\\text{variance}}}\n",
    "\\]\n",
    "\n",
    "Trong đó:\n",
    "- `mean` là giá trị trung bình của dữ liệu huấn luyện.\n",
    "- `variance` là phương sai của dữ liệu huấn luyện.\n",
    "\n",
    "Đây là cách mà dữ liệu đầu vào sẽ được chuyển đổi để có trung bình bằng 0 và độ lệch chuẩn bằng 1.\n",
    "\n",
    "### Ví dụ minh họa\n",
    "\n",
    "Giả sử bạn có một mảng dữ liệu đầu vào như sau:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Dữ liệu giả lập\n",
    "data = np.array([[10, 20, 30],\n",
    "                 [20, 30, 40],\n",
    "                 [30, 40, 50]], dtype=\"float32\")\n",
    "\n",
    "# Khởi tạo lớp Normalization và tính mean và variance\n",
    "normalize = layers.Normalization()\n",
    "normalize.adapt(data)\n",
    "```\n",
    "\n",
    "Lúc này, lớp `Normalization` sẽ tự động tính:\n",
    "- `mean` là trung bình của mỗi cột (tính trên toàn bộ `data`)\n",
    "- `variance` là phương sai của mỗi cột (tính trên toàn bộ `data`)\n",
    "\n",
    "Cụ thể, với `data` trên:\n",
    "- `mean` sẽ là `[20, 30, 40]`\n",
    "- `variance` sẽ là `[66.67, 66.67, 66.67]` (giả sử phương sai là ước lượng gần đúng)\n",
    "\n",
    "Sau đó, bạn có thể chuẩn hóa dữ liệu bằng cách áp dụng lớp `normalize`:\n",
    "\n",
    "```python\n",
    "# Áp dụng lớp normalize\n",
    "normalized_data = normalize(data)\n",
    "print(normalized_data)\n",
    "```\n",
    "\n",
    "Kết quả sẽ là một mảng mới, với các giá trị đã chuẩn hóa:\n",
    "\n",
    "```python\n",
    "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
    "array([[-1.2247449, -1.2247449, -1.2247449],\n",
    "       [ 0.       ,  0.       ,  0.       ],\n",
    "       [ 1.2247449,  1.2247449,  1.2247449]], dtype=float32)>\n",
    "```\n",
    "\n",
    "Giải thích:\n",
    "- Ở hàng đầu tiên, các giá trị được chuẩn hóa thành -1.2247 (tức là thấp hơn `mean` một độ lệch chuẩn).\n",
    "- Ở hàng thứ hai, các giá trị là 0 (trung bình của cột).\n",
    "- Ở hàng cuối cùng, các giá trị là +1.2247 (cao hơn `mean` một độ lệch chuẩn).\n",
    "\n",
    "Kết quả này cho thấy dữ liệu đã được chuẩn hóa, giúp các lớp tiếp theo trong mô hình hoạt động hiệu quả hơn khi huấn luyện."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chuẩn hóa dữ liệu thành trung bình 0 và độ lệch chuẩn 1 giúp làm cho dữ liệu đồng nhất về mặt tỷ lệ giữa các đặc trưng, mang lại lợi ích quan trọng trong việc huấn luyện mô hình:\n",
    "\n",
    "1. **Hội tụ nhanh hơn**: Các đặc trưng có giá trị nằm trong phạm vi nhỏ hơn giúp quá trình tối ưu hóa (như trong thuật toán gradient descent) hội tụ nhanh hơn. Nếu các đặc trưng có phạm vi giá trị khác nhau lớn, thì việc tìm đường đi đến điểm tối ưu sẽ khó khăn hơn vì mô hình có thể nhảy quá mức cần thiết theo chiều có giá trị lớn, và ít thay đổi theo chiều có giá trị nhỏ.\n",
    "\n",
    "2. **Tránh hiện tượng gradient biến thiên không đều**: Chuẩn hóa giảm thiểu sự khác biệt về tỷ lệ của các gradient, giúp thuật toán học cập nhật các tham số đồng đều hơn. Điều này giúp mô hình huấn luyện ổn định, tránh hiện tượng quá trình học không hiệu quả do gradient của một số đặc trưng quá lớn hoặc quá nhỏ.\n",
    "\n",
    "3. **Tránh bị chi phối bởi đặc trưng có giá trị lớn**: Khi các đặc trưng nằm trong cùng khoảng giá trị (khoảng từ -1 đến 1 hoặc tương tự), các đặc trưng có tầm quan trọng tương đối với nhau, không có đặc trưng nào chiếm ưu thế chỉ vì phạm vi giá trị của nó lớn hơn những đặc trưng khác.\n",
    "\n",
    "### Ví dụ minh họa\n",
    "Nếu dữ liệu có một đặc trưng trong khoảng `[0, 1000]` và đặc trưng khác trong khoảng `[0, 1]`, đặc trưng có giá trị lớn hơn sẽ chi phối quá trình huấn luyện. Khi cả hai đặc trưng được chuẩn hóa, chúng sẽ có tầm quan trọng ngang nhau trong việc tính toán trọng số của mô hình, giúp tối ưu hóa một cách công bằng và hiệu quả."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_features_normalized = normalize(abalone_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 7), dtype=float32, numpy=array([[0.523, 0.407, 0.139, 0.825, 0.358, 0.18 , 0.238]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3320, 7), dtype=float32, numpy=\n",
       "array([[-0.724, -0.715, -0.685, ..., -1.   , -0.932, -1.008],\n",
       "       [ 0.514,  0.434, -0.334, ..., -0.014,  0.248, -0.092],\n",
       "       [ 1.092,  1.033,  0.485, ...,  0.172,  0.929,  0.942],\n",
       "       ...,\n",
       "       [ 0.06 ,  0.134, -0.217, ...,  0.076, -0.119,  0.079],\n",
       "       [-1.054, -0.915, -0.803, ..., -1.077, -0.809, -0.844],\n",
       "       [-0.6  , -0.515, -0.451, ..., -1.095, -1.032, -0.556]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_features_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_abalone_model = keras.models.Sequential([\n",
    "    normalize,\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_abalone_model.compile(\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 777us/step - accuracy: 2.3753e-04 - loss: 12.3605\n",
      "Epoch 2/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - accuracy: 6.5958e-04 - loss: 9.5252\n",
      "Epoch 3/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - accuracy: 0.0015 - loss: 5.2651\n",
      "Epoch 4/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - accuracy: 1.5946e-04 - loss: 4.7156\n",
      "Epoch 5/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - accuracy: 1.3739e-04 - loss: 4.8409\n",
      "Epoch 6/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - accuracy: 0.0013 - loss: 4.1258\n",
      "Epoch 7/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - accuracy: 2.6371e-04 - loss: 4.5573\n",
      "Epoch 8/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - accuracy: 9.3831e-04 - loss: 4.3974\n",
      "Epoch 9/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - accuracy: 6.8664e-04 - loss: 4.0650\n",
      "Epoch 10/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - accuracy: 1.7031e-04 - loss: 4.4124\n",
      "Epoch 11/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - accuracy: 9.3912e-05 - loss: 4.2353\n",
      "Epoch 12/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - accuracy: 0.0013 - loss: 4.3032\n",
      "Epoch 13/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - accuracy: 7.1860e-05 - loss: 4.2696\n",
      "Epoch 14/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - accuracy: 2.3511e-05 - loss: 4.3790\n",
      "Epoch 15/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - accuracy: 0.0012 - loss: 4.2928\n",
      "Epoch 16/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - accuracy: 2.7133e-04 - loss: 4.0250\n",
      "Epoch 17/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - accuracy: 2.8498e-04 - loss: 4.0867\n",
      "Epoch 18/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - accuracy: 0.0012 - loss: 3.9232\n",
      "Epoch 19/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - accuracy: 3.5023e-04 - loss: 4.1008\n",
      "Epoch 20/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - accuracy: 2.5695e-04 - loss: 4.0235\n",
      "Epoch 21/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - accuracy: 3.2812e-05 - loss: 4.3666\n",
      "Epoch 22/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - accuracy: 3.9178e-05 - loss: 4.7979\n",
      "Epoch 23/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - accuracy: 2.5033e-04 - loss: 4.1335\n",
      "Epoch 24/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - accuracy: 5.6613e-05 - loss: 4.1597\n",
      "Epoch 25/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - accuracy: 3.0005e-04 - loss: 4.0211\n",
      "Epoch 26/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - accuracy: 8.2919e-04 - loss: 4.4146\n",
      "Epoch 27/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - accuracy: 4.3456e-04 - loss: 4.0099\n",
      "Epoch 28/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - accuracy: 8.6267e-06 - loss: 4.3303\n",
      "Epoch 29/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - accuracy: 3.1038e-05 - loss: 3.9419\n",
      "Epoch 30/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - accuracy: 3.9857e-04 - loss: 4.1055\n",
      "Epoch 31/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - accuracy: 2.8597e-04 - loss: 4.2885\n",
      "Epoch 32/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - accuracy: 8.2919e-04 - loss: 4.0265\n",
      "Epoch 33/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - accuracy: 1.5113e-04 - loss: 4.0312\n",
      "Epoch 34/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - accuracy: 2.1930e-04 - loss: 4.2388\n",
      "Epoch 35/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - accuracy: 6.3478e-04 - loss: 4.0884\n",
      "Epoch 36/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - accuracy: 1.5586e-04 - loss: 4.1007\n",
      "Epoch 37/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - accuracy: 1.1777e-04 - loss: 4.3314\n",
      "Epoch 38/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - accuracy: 1.0359e-04 - loss: 4.0854\n",
      "Epoch 39/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - accuracy: 4.0018e-04 - loss: 4.1379\n",
      "Epoch 40/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - accuracy: 0.0016 - loss: 4.3544\n",
      "Epoch 41/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - accuracy: 0.0015 - loss: 4.2824\n",
      "Epoch 42/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - accuracy: 8.8794e-05 - loss: 4.0577\n",
      "Epoch 43/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - accuracy: 1.1168e-04 - loss: 4.1786\n",
      "Epoch 44/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - accuracy: 9.3831e-04 - loss: 4.1802\n",
      "Epoch 45/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - accuracy: 7.4725e-05 - loss: 4.2856\n",
      "Epoch 46/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - accuracy: 9.8989e-04 - loss: 4.1532\n",
      "Epoch 47/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - accuracy: 1.7838e-04 - loss: 4.1769\n",
      "Epoch 48/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - accuracy: 7.1640e-04 - loss: 4.0224\n",
      "Epoch 49/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - accuracy: 9.3912e-05 - loss: 4.0611\n",
      "Epoch 50/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - accuracy: 2.6579e-05 - loss: 3.8956\n",
      "Epoch 51/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - accuracy: 5.2365e-05 - loss: 4.1062\n",
      "Epoch 52/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - accuracy: 3.2812e-05 - loss: 4.0929\n",
      "Epoch 53/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - accuracy: 2.1149e-04 - loss: 4.0473\n",
      "Epoch 54/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - accuracy: 6.9643e-05 - loss: 3.8683\n",
      "Epoch 55/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - accuracy: 3.3270e-04 - loss: 4.1600\n",
      "Epoch 56/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - accuracy: 2.5033e-04 - loss: 4.2306\n",
      "Epoch 57/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - accuracy: 2.0213e-04 - loss: 4.1213\n",
      "Epoch 58/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - accuracy: 8.4253e-05 - loss: 4.1659\n",
      "Epoch 59/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - accuracy: 2.4565e-04 - loss: 4.2361\n",
      "Epoch 60/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - accuracy: 2.0775e-04 - loss: 3.8166\n",
      "Epoch 61/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - accuracy: 4.1475e-04 - loss: 4.1028\n",
      "Epoch 62/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step - accuracy: 1.5113e-04 - loss: 3.7598\n",
      "Epoch 63/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - accuracy: 5.1477e-04 - loss: 3.8572\n",
      "Epoch 64/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - accuracy: 2.1347e-04 - loss: 4.0320\n",
      "Epoch 65/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 4.4696e-04 - loss: 4.0009\n",
      "Epoch 66/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - accuracy: 5.5717e-05 - loss: 4.0088\n",
      "Epoch 67/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - accuracy: 1.5765e-04 - loss: 4.1652\n",
      "Epoch 68/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - accuracy: 5.3192e-04 - loss: 3.9029\n",
      "Epoch 69/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - accuracy: 3.5047e-04 - loss: 3.9731\n",
      "Epoch 70/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - accuracy: 9.3831e-04 - loss: 4.0128\n",
      "Epoch 71/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step - accuracy: 1.5113e-04 - loss: 3.8642\n",
      "Epoch 72/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - accuracy: 5.8032e-05 - loss: 4.1374\n",
      "Epoch 73/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - accuracy: 2.1930e-04 - loss: 4.1266\n",
      "Epoch 74/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - accuracy: 7.4038e-05 - loss: 4.0810\n",
      "Epoch 75/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - accuracy: 2.1930e-04 - loss: 3.9384\n",
      "Epoch 76/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - accuracy: 3.5306e-05 - loss: 4.2798\n",
      "Epoch 77/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - accuracy: 9.5701e-05 - loss: 4.0878\n",
      "Epoch 78/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - accuracy: 8.2919e-04 - loss: 3.8663\n",
      "Epoch 79/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - accuracy: 4.4696e-04 - loss: 4.0852\n",
      "Epoch 80/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - accuracy: 7.4947e-04 - loss: 4.1156\n",
      "Epoch 81/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - accuracy: 4.4653e-04 - loss: 3.7472\n",
      "Epoch 82/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - accuracy: 5.7883e-05 - loss: 4.3454\n",
      "Epoch 83/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - accuracy: 2.5604e-05 - loss: 3.9616\n",
      "Epoch 84/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - accuracy: 2.3753e-04 - loss: 3.8349\n",
      "Epoch 85/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - accuracy: 0.0015 - loss: 4.1657\n",
      "Epoch 86/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - accuracy: 2.1347e-04 - loss: 3.8518\n",
      "Epoch 87/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - accuracy: 4.2548e-04 - loss: 4.1315\n",
      "Epoch 88/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - accuracy: 8.2919e-04 - loss: 3.9047\n",
      "Epoch 89/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - accuracy: 2.2858e-05 - loss: 3.7870\n",
      "Epoch 90/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - accuracy: 3.4145e-04 - loss: 3.8440\n",
      "Epoch 91/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - accuracy: 1.7622e-04 - loss: 3.9099\n",
      "Epoch 92/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - accuracy: 3.0005e-04 - loss: 4.0356\n",
      "Epoch 93/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - accuracy: 2.0474e-04 - loss: 3.8458\n",
      "Epoch 94/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - accuracy: 7.3301e-04 - loss: 4.1740\n",
      "Epoch 95/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - accuracy: 3.1593e-04 - loss: 3.8975\n",
      "Epoch 96/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - accuracy: 4.7343e-04 - loss: 3.9590\n",
      "Epoch 97/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - accuracy: 1.8589e-04 - loss: 4.1521\n",
      "Epoch 98/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - accuracy: 5.5717e-05 - loss: 4.1353\n",
      "Epoch 99/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - accuracy: 5.7079e-04 - loss: 4.1782\n",
      "Epoch 100/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - accuracy: 1.0359e-04 - loss: 3.9854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23bc9e84ca0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_abalone_model.fit(abalone_features, abalone_labels, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 - 0s - 985us/step - accuracy: 0.0000e+00 - loss: 6.7513\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = abalone_model.evaluate(abalone_features_test, abalone_labels_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.751287460327148"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0634e-04 - loss: 86.8551 - val_accuracy: 0.0000e+00 - val_loss: 29.9429\n",
      "Epoch 2/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - accuracy: 7.7572e-04 - loss: 27.6563 - val_accuracy: 0.0000e+00 - val_loss: 13.4768\n",
      "Epoch 3/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 3.5551e-04 - loss: 12.9791 - val_accuracy: 0.0000e+00 - val_loss: 6.7564\n",
      "Epoch 4/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - accuracy: 1.3501e-05 - loss: 7.3426 - val_accuracy: 0.0000e+00 - val_loss: 6.1900\n",
      "Epoch 5/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978us/step - accuracy: 5.1937e-05 - loss: 6.5209 - val_accuracy: 0.0000e+00 - val_loss: 5.8137\n",
      "Epoch 6/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 3.3330e-04 - loss: 6.9335 - val_accuracy: 0.0000e+00 - val_loss: 5.6219\n",
      "Epoch 7/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step - accuracy: 7.9402e-04 - loss: 5.5207 - val_accuracy: 0.0000e+00 - val_loss: 5.4318\n",
      "Epoch 8/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - accuracy: 1.7009e-04 - loss: 5.5109 - val_accuracy: 0.0000e+00 - val_loss: 5.0331\n",
      "Epoch 9/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - accuracy: 1.9437e-04 - loss: 5.0745 - val_accuracy: 0.0000e+00 - val_loss: 4.9379\n",
      "Epoch 10/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 6.6642e-04 - loss: 5.1027 - val_accuracy: 0.0000e+00 - val_loss: 4.7258\n",
      "Epoch 11/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - accuracy: 2.3035e-04 - loss: 4.7528 - val_accuracy: 0.0000e+00 - val_loss: 4.6654\n",
      "Epoch 12/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - accuracy: 5.2693e-04 - loss: 4.6727 - val_accuracy: 0.0000e+00 - val_loss: 4.6485\n",
      "Epoch 13/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.7998e-04 - loss: 4.4958 - val_accuracy: 0.0000e+00 - val_loss: 4.5819\n",
      "Epoch 14/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 7.5768e-04 - loss: 4.2681 - val_accuracy: 0.0000e+00 - val_loss: 4.5223\n",
      "Epoch 15/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - accuracy: 2.8295e-04 - loss: 4.5308 - val_accuracy: 0.0000e+00 - val_loss: 4.6174\n",
      "Epoch 16/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - accuracy: 4.6062e-04 - loss: 4.6518 - val_accuracy: 0.0000e+00 - val_loss: 4.4324\n",
      "Epoch 17/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 3.9154e-04 - loss: 4.4580 - val_accuracy: 0.0000e+00 - val_loss: 4.4584\n",
      "Epoch 18/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - accuracy: 7.1090e-04 - loss: 4.6258 - val_accuracy: 0.0000e+00 - val_loss: 4.4151\n",
      "Epoch 19/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - accuracy: 5.2693e-04 - loss: 3.9488 - val_accuracy: 0.0000e+00 - val_loss: 4.4056\n",
      "Epoch 20/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - accuracy: 4.4574e-04 - loss: 4.7375 - val_accuracy: 0.0000e+00 - val_loss: 4.3425\n",
      "Epoch 21/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - accuracy: 1.8710e-04 - loss: 4.6343 - val_accuracy: 0.0000e+00 - val_loss: 4.3553\n",
      "Epoch 22/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - accuracy: 2.6458e-04 - loss: 4.2508 - val_accuracy: 0.0000e+00 - val_loss: 4.5125\n",
      "Epoch 23/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - accuracy: 1.7009e-04 - loss: 4.3238 - val_accuracy: 0.0000e+00 - val_loss: 4.3922\n",
      "Epoch 24/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - accuracy: 1.5644e-04 - loss: 4.3928 - val_accuracy: 0.0000e+00 - val_loss: 4.3487\n",
      "Epoch 25/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896us/step - accuracy: 2.1434e-04 - loss: 4.2525 - val_accuracy: 0.0000e+00 - val_loss: 4.3098\n",
      "Epoch 26/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 3.5681e-04 - loss: 4.3246 - val_accuracy: 0.0000e+00 - val_loss: 4.6605\n",
      "Epoch 27/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - accuracy: 6.2201e-05 - loss: 4.2138 - val_accuracy: 0.0000e+00 - val_loss: 4.3337\n",
      "Epoch 28/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 8.1293e-04 - loss: 4.5033 - val_accuracy: 0.0000e+00 - val_loss: 4.3931\n",
      "Epoch 29/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - accuracy: 5.0921e-04 - loss: 4.0460 - val_accuracy: 0.0000e+00 - val_loss: 4.2744\n",
      "Epoch 30/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - accuracy: 6.3091e-04 - loss: 4.1216 - val_accuracy: 0.0000e+00 - val_loss: 4.2900\n",
      "Epoch 31/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step - accuracy: 2.3035e-04 - loss: 4.1919 - val_accuracy: 0.0000e+00 - val_loss: 4.2543\n",
      "Epoch 32/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step - accuracy: 1.9900e-04 - loss: 4.2058 - val_accuracy: 0.0000e+00 - val_loss: 4.3432\n",
      "Epoch 33/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - accuracy: 1.5280e-04 - loss: 4.4718 - val_accuracy: 0.0000e+00 - val_loss: 4.2974\n",
      "Epoch 34/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 2.4132e-04 - loss: 4.0111 - val_accuracy: 0.0000e+00 - val_loss: 4.2933\n",
      "Epoch 35/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step - accuracy: 0.0011 - loss: 4.1178 - val_accuracy: 0.0000e+00 - val_loss: 4.2759\n",
      "Epoch 36/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 5.6511e-04 - loss: 4.3116 - val_accuracy: 0.0000e+00 - val_loss: 4.2221\n",
      "Epoch 37/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 906us/step - accuracy: 2.5545e-04 - loss: 4.0299 - val_accuracy: 0.0000e+00 - val_loss: 4.2355\n",
      "Epoch 38/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - accuracy: 2.7365e-04 - loss: 3.8675 - val_accuracy: 0.0000e+00 - val_loss: 4.2971\n",
      "Epoch 39/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 5.6511e-04 - loss: 4.2179 - val_accuracy: 0.0000e+00 - val_loss: 4.2301\n",
      "Epoch 40/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - accuracy: 2.0744e-04 - loss: 3.9772 - val_accuracy: 0.0000e+00 - val_loss: 4.3177\n",
      "Epoch 41/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 1.1825e-04 - loss: 4.2513 - val_accuracy: 0.0000e+00 - val_loss: 4.2966\n",
      "Epoch 42/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - accuracy: 2.6815e-05 - loss: 3.9200 - val_accuracy: 0.0000e+00 - val_loss: 4.3026\n",
      "Epoch 43/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - accuracy: 1.3055e-04 - loss: 4.0960 - val_accuracy: 0.0000e+00 - val_loss: 4.2613\n",
      "Epoch 44/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - accuracy: 1.3501e-05 - loss: 4.1689 - val_accuracy: 0.0000e+00 - val_loss: 4.4191\n",
      "Epoch 45/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - accuracy: 0.0015 - loss: 4.2286 - val_accuracy: 0.0000e+00 - val_loss: 4.2372\n",
      "Epoch 46/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step - accuracy: 6.0915e-04 - loss: 3.7653 - val_accuracy: 0.0000e+00 - val_loss: 4.2347\n",
      "Epoch 47/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - accuracy: 1.5644e-04 - loss: 4.2083 - val_accuracy: 0.0000e+00 - val_loss: 4.2301\n",
      "Epoch 48/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - accuracy: 6.2201e-05 - loss: 4.2121 - val_accuracy: 0.0000e+00 - val_loss: 4.2128\n",
      "Epoch 49/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - accuracy: 7.7572e-04 - loss: 4.0296 - val_accuracy: 0.0000e+00 - val_loss: 4.2058\n",
      "Epoch 50/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - accuracy: 3.3330e-04 - loss: 4.0432 - val_accuracy: 0.0000e+00 - val_loss: 4.2282\n",
      "Epoch 51/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.7711e-04 - loss: 3.9956 - val_accuracy: 0.0000e+00 - val_loss: 4.4170\n",
      "Epoch 52/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - accuracy: 2.7454e-05 - loss: 3.8453 - val_accuracy: 0.0000e+00 - val_loss: 4.2418\n",
      "Epoch 53/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968us/step - accuracy: 2.0005e-04 - loss: 4.1156 - val_accuracy: 0.0000e+00 - val_loss: 4.1816\n",
      "Epoch 54/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - accuracy: 4.8085e-04 - loss: 4.2693 - val_accuracy: 0.0000e+00 - val_loss: 4.1988\n",
      "Epoch 55/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 8.3618e-05 - loss: 4.1766 - val_accuracy: 0.0000e+00 - val_loss: 4.1911\n",
      "Epoch 56/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - accuracy: 8.1293e-04 - loss: 4.3600 - val_accuracy: 0.0000e+00 - val_loss: 4.3776\n",
      "Epoch 57/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - accuracy: 3.9154e-04 - loss: 4.3838 - val_accuracy: 0.0000e+00 - val_loss: 4.2386\n",
      "Epoch 58/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.5578e-04 - loss: 4.1079 - val_accuracy: 0.0000e+00 - val_loss: 4.2317\n",
      "Epoch 59/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 2.8569e-04 - loss: 4.1897 - val_accuracy: 0.0000e+00 - val_loss: 4.2500\n",
      "Epoch 60/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step - accuracy: 1.4979e-04 - loss: 4.1161 - val_accuracy: 0.0000e+00 - val_loss: 4.2155\n",
      "Epoch 61/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - accuracy: 2.9249e-04 - loss: 4.0700 - val_accuracy: 0.0000e+00 - val_loss: 4.2272\n",
      "Epoch 62/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 2.3035e-04 - loss: 4.0214 - val_accuracy: 0.0000e+00 - val_loss: 4.2217\n",
      "Epoch 63/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - accuracy: 1.3367e-04 - loss: 3.9526 - val_accuracy: 0.0000e+00 - val_loss: 4.1783\n",
      "Epoch 64/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 4.8606e-04 - loss: 3.8799 - val_accuracy: 0.0000e+00 - val_loss: 4.2431\n",
      "Epoch 65/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 3.6714e-04 - loss: 4.2695 - val_accuracy: 0.0000e+00 - val_loss: 4.2199\n",
      "Epoch 66/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - accuracy: 3.5551e-04 - loss: 4.0210 - val_accuracy: 0.0000e+00 - val_loss: 4.2279\n",
      "Epoch 67/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - accuracy: 2.7365e-04 - loss: 4.2412 - val_accuracy: 0.0000e+00 - val_loss: 4.1774\n",
      "Epoch 68/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - accuracy: 4.0325e-04 - loss: 3.6623 - val_accuracy: 0.0000e+00 - val_loss: 4.2256\n",
      "Epoch 69/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 7.0470e-04 - loss: 3.9243 - val_accuracy: 0.0000e+00 - val_loss: 4.1731\n",
      "Epoch 70/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step - accuracy: 1.7009e-04 - loss: 3.8451 - val_accuracy: 0.0000e+00 - val_loss: 4.3113\n",
      "Epoch 71/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 3.6714e-04 - loss: 4.0365 - val_accuracy: 0.0000e+00 - val_loss: 4.1992\n",
      "Epoch 72/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - accuracy: 0.0013 - loss: 3.7143 - val_accuracy: 0.0000e+00 - val_loss: 4.3626\n",
      "Epoch 73/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - accuracy: 1.2435e-04 - loss: 4.0521 - val_accuracy: 0.0000e+00 - val_loss: 4.2481\n",
      "Epoch 74/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - accuracy: 2.7454e-05 - loss: 4.0505 - val_accuracy: 0.0000e+00 - val_loss: 4.2909\n",
      "Epoch 75/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978us/step - accuracy: 3.8244e-04 - loss: 3.8590 - val_accuracy: 0.0000e+00 - val_loss: 4.1896\n",
      "Epoch 76/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step - accuracy: 2.2744e-05 - loss: 3.9666 - val_accuracy: 0.0000e+00 - val_loss: 4.2631\n",
      "Epoch 77/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - accuracy: 0.0010 - loss: 4.1675 - val_accuracy: 0.0000e+00 - val_loss: 4.5799\n",
      "Epoch 78/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - accuracy: 0.0012 - loss: 4.2741 - val_accuracy: 0.0000e+00 - val_loss: 4.3701\n",
      "Epoch 79/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 6.7440e-05 - loss: 4.0107 - val_accuracy: 0.0000e+00 - val_loss: 4.2149\n",
      "Epoch 80/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0019 - loss: 3.9893 - val_accuracy: 0.0000e+00 - val_loss: 4.2142\n",
      "Epoch 81/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - accuracy: 0.0012 - loss: 3.8597 - val_accuracy: 0.0000e+00 - val_loss: 4.1988\n",
      "Epoch 82/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - accuracy: 1.2435e-04 - loss: 4.0136 - val_accuracy: 0.0000e+00 - val_loss: 4.2007\n",
      "Epoch 83/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.3685e-04 - loss: 4.0014 - val_accuracy: 0.0000e+00 - val_loss: 4.2563\n",
      "Epoch 84/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 2.0659e-04 - loss: 3.8163 - val_accuracy: 0.0000e+00 - val_loss: 4.2572\n",
      "Epoch 85/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - accuracy: 4.0437e-04 - loss: 3.9317 - val_accuracy: 0.0000e+00 - val_loss: 4.3305\n",
      "Epoch 86/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 7.4595e-05 - loss: 3.9777 - val_accuracy: 0.0000e+00 - val_loss: 4.3564\n",
      "Epoch 87/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - accuracy: 2.4707e-04 - loss: 4.0592 - val_accuracy: 0.0000e+00 - val_loss: 4.2633\n",
      "Epoch 88/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969us/step - accuracy: 2.6729e-04 - loss: 3.9477 - val_accuracy: 0.0000e+00 - val_loss: 4.2051\n",
      "Epoch 89/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 973us/step - accuracy: 6.3652e-05 - loss: 3.9432 - val_accuracy: 0.0000e+00 - val_loss: 4.2399\n",
      "Epoch 90/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986us/step - accuracy: 3.2267e-04 - loss: 4.1388 - val_accuracy: 0.0000e+00 - val_loss: 4.1578\n",
      "Epoch 91/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step - accuracy: 5.3284e-04 - loss: 4.1297 - val_accuracy: 0.0000e+00 - val_loss: 4.1693\n",
      "Epoch 92/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 984us/step - accuracy: 8.3618e-05 - loss: 3.9226 - val_accuracy: 0.0000e+00 - val_loss: 4.1858\n",
      "Epoch 93/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - accuracy: 0.0010 - loss: 3.8015 - val_accuracy: 0.0000e+00 - val_loss: 4.2944\n",
      "Epoch 94/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - accuracy: 2.1434e-04 - loss: 3.7564 - val_accuracy: 0.0000e+00 - val_loss: 4.1513\n",
      "Epoch 95/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - accuracy: 1.7998e-04 - loss: 4.0060 - val_accuracy: 0.0000e+00 - val_loss: 4.1750\n",
      "Epoch 96/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - accuracy: 2.2226e-04 - loss: 3.9573 - val_accuracy: 0.0000e+00 - val_loss: 4.1934\n",
      "Epoch 97/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step - accuracy: 1.4979e-04 - loss: 3.9456 - val_accuracy: 0.0000e+00 - val_loss: 4.1847\n",
      "Epoch 98/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - accuracy: 3.1154e-04 - loss: 4.0368 - val_accuracy: 0.0000e+00 - val_loss: 4.2427\n",
      "Epoch 99/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 8.9481e-05 - loss: 3.7098 - val_accuracy: 0.0000e+00 - val_loss: 4.2152\n",
      "Epoch 100/100\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 6.9550e-04 - loss: 3.8927 - val_accuracy: 0.0000e+00 - val_loss: 4.3050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23bccb11070>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    layers.Input(shape=(7,)),  # Đầu vào với 7 đặc trưng\n",
    "    normalize,\n",
    "    layers.Dense(64, activation='relu'),  # Lớp ẩn đầu tiên với 64 nơ-ron\n",
    "    layers.Dense(32, activation='relu'),  # Lớp ẩn thứ hai với 32 nơ-ron\n",
    "    layers.Dense(1)  # Lớp đầu ra cho giá trị tuổi (regression)\n",
    "])\n",
    "\n",
    "# Biên dịch mô hình\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',  # Sử dụng MSE cho bài toán hồi quy\n",
    "              metrics=['accuracy'])  # Mean Absolute Error để đánh giá mô hình\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "model.fit(abalone_features, abalone_labels, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 - 0s - 1ms/step - accuracy: 0.0000e+00 - loss: 5.1988\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(abalone_features_test, abalone_labels_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
