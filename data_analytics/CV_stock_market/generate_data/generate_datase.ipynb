{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from pyspark import  SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from ta import add_all_ta_features\n",
    "from ta.utils import dropna\n",
    "from utils.draw_candle_image import *\n",
    "import json\n",
    "import keras\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import ta\n",
    "import ta.momentum\n",
    "import ta.trend\n",
    "import ta.trend\n",
    "import ta.volatility\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = \"fpt\"\n",
    "file_csv = f'../data/{symbol}/fpt_2006_12_13_to_2024_11_15.csv'\n",
    "stock_df = pd.read_csv(file_csv)\n",
    "stock_df[\"Date\"] = pd.to_datetime(stock_df['Date'])\n",
    "stock_df.sort_values(by=\"Date\", ascending=True, inplace=True)\n",
    "stock_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "stock_df[\"Macd\"] = ta.trend.macd(stock_df[\"Close\"], fillna=True)\n",
    "stock_df[\"Macd_signal\"]= ta.trend.macd_signal(stock_df[\"Close\"], fillna=True)\n",
    "stock_df[\"Macd_histogram\"] = ta.trend.macd_diff(stock_df[\"Close\"], fillna=True)\n",
    "\n",
    "indicator_bb = ta.volatility.BollingerBands(close=stock_df[\"Close\"], window=20, window_dev=2, fillna=True)\n",
    "stock_df['BB_avg'] = indicator_bb.bollinger_mavg()\n",
    "stock_df['BB_high'] = indicator_bb.bollinger_hband()\n",
    "stock_df['BB_low'] = indicator_bb.bollinger_lband()\n",
    "\n",
    "indicator_SMA = ta.trend.SMAIndicator(stock_df[\"Close\"], 20, fillna=True)\n",
    "stock_df[\"SMA\"] = indicator_SMA.sma_indicator()\n",
    "\n",
    "indicator_EMA_50 = ta.trend.EMAIndicator(stock_df[\"Close\"], 50, fillna=True)\n",
    "stock_df[\"EMA_50\"] = indicator_EMA_50.ema_indicator()\n",
    "\n",
    "indicator_EMA_200 = ta.trend.EMAIndicator(stock_df[\"Close\"], 200, fillna=True)\n",
    "stock_df[\"EMA_200\"] = indicator_EMA_200.ema_indicator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = stock_df[\"Date\"].dt.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_year = 2006\n",
    "max_year = 2020\n",
    "take_years = list(range(min_year, max_year))\n",
    "years_to_test = {\n",
    "    2019: list(range(1, 13)),\n",
    "    2020: list(range(1, 13)),\n",
    "}\n",
    "years = take_years\n",
    "list_years_to_test_str = \"_\".join([str(year) for year in years_to_test.keys()])\n",
    "symbol = os.path.basename(file_csv).split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figscale = 0.5\n",
    "previous_days = 0\n",
    "next_days = 2\n",
    "days_result = 3\n",
    "type_dataset = \"with_trend_type\"\n",
    "folder_save = f\"../dataset/{symbol}_dataset_{previous_days}_{next_days}_{days_result}_{np.min(years)}_{np.max(years)}_test_{list_years_to_test_str}_{type_dataset}\"\n",
    "while os.path.exists(folder_save):\n",
    "    continue_with_name = input(\"append name: \")\n",
    "    folder_save += continue_with_name\n",
    "os.makedirs(f\"{folder_save}/model_save\", exist_ok=True)\n",
    "os.makedirs(f\"{folder_save}/output_prediction_image\", exist_ok=True)\n",
    "\n",
    "indicator = {\n",
    "    \"show_macd\": False,\n",
    "    \"show_macd_signal\": False,\n",
    "    \"show_macd_histogram\": False,\n",
    "\n",
    "    \"show_BB_avg\": False,\n",
    "    \"show_BB_high\": False,\n",
    "    \"show_BB_low\": False,\n",
    "\n",
    "    \"show_SMA\": False,\n",
    "    \"show_EMA_50\": False,\n",
    "    \"show_EMA_200\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_df[stock_df[\"Date\"].dt.year == 2014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_candle_image(\n",
    "#     add_days_around_date(\"2019-09-21\", stock_df, previous_days, next_days), \n",
    "#     show_x_y=False, \n",
    "#     show_volume=False, \n",
    "    \n",
    "#     **indicator,\n",
    "    \n",
    "#     figscale=1, \n",
    "#     figcolor=\"black\", \n",
    "#     preview_image=True,\n",
    "#     return_image_tensor=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = r'C:\\Program Files\\Java\\jre-1.8'\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\" \n",
    "spark: SparkSession = SparkSession.builder.master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_spark = spark.createDataFrame(stock_df)\n",
    "data_stock = \"data_stock\"\n",
    "data_df_spark.createOrReplaceTempView(data_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stock_sub_1 = \"data_stock_sub_1\"\n",
    "data_stock_include_previous_date = \"data_stock_include_previous_date\"\n",
    "bullish_engulfing = \"bullish_engulfing\"\n",
    "data_stock_sub_2 = \"data_stock_sub_2\"\n",
    "data_stock_sub_3 = \"data_stock_sub_3\"\n",
    "bearish_engulfing = \"bearish_engulfing\"\n",
    "data_stock_sub_4 = \"data_stock_sub_4\"\n",
    "data_stock_include_past_and_future_date = \"data_stock_include_past_and_future_date\"\n",
    "data_stock_include_engulfing = \"data_stock_include_engulfing\"\n",
    "recent_engulfing_type = \"days_start_engulfing\"\n",
    "data_stock_day_bearish_engulfing = \"data_stock_day_bearish_engulfing\"\n",
    "data_stock_include_group_engulfing = \"data_stock_include_group_engulfing\"\n",
    "data_stock_include_group_engulfing_sub_1 = \"data_stock_include_group_engulfing_sub_1\"\n",
    "start_and_end_date_of_group_engulfing = \"start_and_end_date_of_group_engulfing\"\n",
    "data_stock_include_harami = \"data_stock_include_harami\"\n",
    "data_stock_include_tweezer = \"data_stock_include_tweezer\"\n",
    "data_stock_include_morning_and_evening_star = \"data_stock_include_morning_and_evening_star\"\n",
    "data_stock_include_next_3_day = \"data_stock_include_next_3_day\"\n",
    "recent_next_3_day_type = \"recent_next_3_day_type\"\n",
    "data_stock_include_group_next_3_day = \"data_stock_include_group_next_3_day\"\n",
    "data_stock_include_group_next_3_day_sub_1 = \"data_stock_include_group_next_3_day_sub_1\"\n",
    "data_stock_include_group_sideway = \"data_stock_include_group_sideway\"\n",
    "last_candle_index_in_group_next_3_day = \"last_candle_index_in_group_next_3_day\"\n",
    "last_candle_index_in_group_next_3_day_sub_1 = \"last_candle_index_in_group_next_3_day_sub_1\"\n",
    "data_stock_base = \"data_stock_base\"\n",
    "\n",
    "Date = \"Date\"\n",
    "Close = \"Close\"\n",
    "Open = \"Open\"\n",
    "High = \"High\"\n",
    "Low = \"Low\"\n",
    "Macd = \"Macd\"\n",
    "Macd_signal = \"Macd_signal\"\n",
    "Macd_histogram = \"Macd_histogram\"\n",
    "\n",
    "Date_normalized = \"Date_normalized\"\n",
    "Body_height = \"Body_height\"\n",
    "Max_body_height_in_one_month_around = \"Max_body_height_in_one_month_around\"\n",
    "Max_of_candle_body = \"Max_of_candle_body\"\n",
    "Min_of_candle_body = \"Min_of_candle_body\"\n",
    "Is_star_doji_candle = \"Is_star_doji_candle\"\n",
    "Is_candle_up = \"Is_candle_up\"\n",
    "Is_narrow_body_candle = \"Is_narrow_body_candle\"\n",
    "Is_dragonfly_doji_candle = \"Is_dragonfly_doji_candle\"\n",
    "Is_gravestone_doji_candle = \"Is_gravestone_doji_candle\"\n",
    "\n",
    "Previous_date = \"Previous_date\"\n",
    "Close_of_previous_day = \"Close_of_previous_day\"\n",
    "Open_of_previous_day = \"Open_of_previous_day\"\n",
    "High_of_previous_day = \"High_of_previous_day\"\n",
    "Low_of_previous_day = \"Low_of_previous_day\"\n",
    "Is_previous_day_candle_up = \"Is_previous_day_candle_up\"\n",
    "Body_height_of_previous_day = \"Body_height_of_previous_day\"\n",
    "Is_previous_day_start_doji_candle = \"Is_previous_day_start_doji_candle\"\n",
    "Max_of_candle_body_of_previous_day = \"Max_of_candle_body_of_previous_day\"\n",
    "Min_of_candle_body_of_previous_day = \"Min_of_candle_body_of_previous_day\"\n",
    "\n",
    "Next_date = \"Next_date\"\n",
    "Close_of_next_day = \"Close_of_next_day\"\n",
    "Open_of_next_day = \"Open_of_next_day\"\n",
    "High_of_next_day = \"High_of_next_day\"\n",
    "Low_of_next_day = \"Low_of_next_day\"\n",
    "Is_next_day_candle_up = \"Is_next_day_candle_up\"\n",
    "Is_next_day_start_doji_candle = \"Is_next_day_start_doji_candle\"\n",
    "Body_height_of_next_day = \"Body_height_of_next_day\"\n",
    "Max_of_candle_body_of_next_day = \"Max_of_candle_body_of_next_day\"\n",
    "Min_of_candle_body_of_next_day = \"Min_of_candle_body_of_next_day\"\n",
    "\n",
    "Is_start_bullish_engulfing_candle = \"Is_start_bullish_engulfing_candle\"\n",
    "Is_start_bearish_engulfing_candle = \"Is_start_bearish_engulfing_candle\"\n",
    "Is_recent_candle_is_bullish_engulfing = \"Is_recent_candle_is_bullish_engulfing\"\n",
    "Is_recent_candle_is_bearish_engulfing = \"Is_recent_candle_is_bearish_engulfing\"\n",
    "\n",
    "Group_id_engulfing = \"Group_id_engulfing\"\n",
    "Start_date_of_group = \"Start_date_of_group\"\n",
    "End_date_of_group = \"End_date_of_group\"\n",
    "Number_days_in_group = \"Number_days_in_group\"\n",
    "Group_engulfing_type = \"Group_engulfing_type\"\n",
    "Start_engulfing_candle_type = \"Start_engulfing_candle_type\"\n",
    "\n",
    "GROUP_BULL = \"GROUP_BULL\"\n",
    "GROUP_BEAR = \"GROUP_BEAR\"\n",
    "GROUP_SIZEWAY = \"GROUP_SIZEWAY\"\n",
    "\n",
    "Is_start_harami_up_candle = \"Is_start_harami_up_candle\"\n",
    "Is_start_harami_down_candle = \"Is_start_harami_down_candle\"\n",
    "\n",
    "Is_start_tweezer_top_candle = \"Is_start_tweezer_top_candle\"\n",
    "Is_start_tweezer_bottom_candle = \"Is_start_tweezer_bottom_candle\"\n",
    "\n",
    "Is_marubozu_candle = \"Is_marubozu_candle\"\n",
    "\n",
    "Next_2_date = \"Next_2_date\"\n",
    "Close_of_next_2_day = \"Close_of_next_2_day\"\n",
    "Open_of_next_2_day = \"Open_of_next_2_day\"\n",
    "High_of_next_2_day = \"High_of_next_2_day\"\n",
    "Low_of_next_2_day = \"Low_of_next_2_day\"\n",
    "Is_next_2_day_candle_up = \"Is_next_2_day_candle_up\"\n",
    "Is_next_2_day_start_doji_candle = \"Is_next_2_day_start_doji_candle\"\n",
    "Body_height_of_next_2_day = \"Body_height_of_next_2_day\"\n",
    "Max_of_candle_body_of_next_2_day = \"Max_of_candle_body_of_next_2_day\"\n",
    "Min_of_candle_body_of_next_2_day = \"Min_of_candle_body_of_next_2_day\"\n",
    "\n",
    "\n",
    "Is_start_morning_star_candle = \"Is_start_morning_star_candle\"\n",
    "Is_start_evening_star_candle = \"Is_start_evening_star_candle\"\n",
    "\n",
    "Is_hammer_candle = \"Is_hammer_candle\"\n",
    "Is_inverted_hammer_candle = \"Is_inverted_hammer_candle\"\n",
    "\n",
    "Is_spin_candle = \"Is_spin_candle\"\n",
    "\n",
    "Is_start_next_3_day_up_candle = \"Is_start_next_3_day_up_candle\"\n",
    "Is_start_next_3_day_down_candle = \"Is_start_next_3_day_down_candle\"\n",
    "\n",
    "Total_records = \"Total_records\"\n",
    "\n",
    "Is_recent_candle_is_next_3_day_up = \"Is_recent_candle_is_next_3_day_up\"\n",
    "Is_recent_candle_is_next_3_day_down = \"Is_recent_candle_is_next_3_day_down\"\n",
    "\n",
    "Group_id_next_3_day = \"Group_id_next_3_day\"\n",
    "Start_next_3_day_candle_type = \"Start_next_3_day_candle_type\"\n",
    "Group_next_3_day_type = \"Group_next_3_day_type\"\n",
    "\n",
    "\n",
    "Group_trend_3_day_type = \"Group_trend_3_day_type\"\n",
    "Index_in_group_next_3_day = \"Index_in_group_next_3_day\"\n",
    "Max_index_in_group_next_3_day = \"Max_index_in_group_next_3_day\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_query = f\"\"\"\n",
    "    WITH\n",
    "    {data_stock_sub_1} AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            CASE\n",
    "                WHEN \n",
    "                    {Is_narrow_body_candle} \n",
    "                    AND ABS({High} - {Max_of_candle_body}) / ABS({Low} - {Min_of_candle_body}) * 100 BETWEEN 90 AND 110 \n",
    "                THEN True\n",
    "                ELSE False\n",
    "            END AS {Is_star_doji_candle},\n",
    "            CASE\n",
    "                WHEN\n",
    "                    {Is_narrow_body_candle} \n",
    "                    AND ABS({High} - {Max_of_candle_body}) / ABS({Low} - {Min_of_candle_body}) * 100 <= 10\n",
    "                THEN True\n",
    "                ELSE False\n",
    "            END AS {Is_dragonfly_doji_candle},\n",
    "            CASE\n",
    "                WHEN\n",
    "                    {Is_narrow_body_candle} \n",
    "                    AND ABS({Low} - {Min_of_candle_body}) / ABS({High} - {Max_of_candle_body}) * 100 <= 10\n",
    "                THEN True\n",
    "                ELSE False\n",
    "            END AS {Is_gravestone_doji_candle},\n",
    "            CASE\n",
    "                WHEN\n",
    "                    ABS({High} - {Max_of_candle_body}) / {Body_height} * 100 <= 2\n",
    "                    AND ABS({Low} - {Min_of_candle_body}) / {Body_height} * 100 <= 2\n",
    "                THEN True\n",
    "                ELSE False\n",
    "            END AS {Is_marubozu_candle},\n",
    "            CASE\n",
    "                WHEN \n",
    "                    ABS({Low} - {Min_of_candle_body}) / {Body_height} >= 2\n",
    "                    AND ABS({High} - {Max_of_candle_body}) / {Body_height} * 100 <= 2\n",
    "                THEN True\n",
    "                ELSE False\n",
    "            END AS {Is_hammer_candle},\n",
    "            CASE\n",
    "                WHEN \n",
    "                    ABS({High} - {Max_of_candle_body}) / {Body_height} >= 2\n",
    "                    AND ABS({Low} - {Min_of_candle_body}) / {Body_height} * 100 <= 2\n",
    "                THEN True\n",
    "                ELSE False\n",
    "            END AS {Is_inverted_hammer_candle},\n",
    "            CASE\n",
    "                WHEN \n",
    "                    {Body_height} / {Max_body_height_in_one_month_around} * 100 <= 20\n",
    "                    AND ABS({High} - {Max_of_candle_body}) / ABS({Low} - {Min_of_candle_body}) * 100 BETWEEN 90 AND 110 \n",
    "                THEN True\n",
    "                ELSE False\n",
    "            END AS {Is_spin_candle}\n",
    "        FROM (\n",
    "            SELECT\n",
    "                *,\n",
    "                CASE\n",
    "                    WHEN\n",
    "                        {Body_height} / {Max_body_height_in_one_month_around} * 100 <= 2 THEN True\n",
    "                    ELSE False\n",
    "                END AS {Is_narrow_body_candle}\n",
    "            FROM (\n",
    "                SELECT\n",
    "                    *,\n",
    "                    MAX({Body_height}) OVER(ORDER BY {Date_normalized} ROWS BETWEEN 15 PRECEDING AND 15 FOLLOWING) AS {Max_body_height_in_one_month_around}\n",
    "                FROM (\n",
    "                    SELECT\n",
    "                        TO_DATE(CAST({Date} AS TIMESTAMP)) AS {Date_normalized},\n",
    "                        {Close},\n",
    "                        {Open},\n",
    "                        {High},\n",
    "                        {Low},\n",
    "                        {Macd},\n",
    "                        {Macd_signal},\n",
    "                        {Macd_histogram},\n",
    "                        CASE\n",
    "                            WHEN {Close} < {Open} THEN False\n",
    "                            ELSE True\n",
    "                        END AS {Is_candle_up},\n",
    "                        GREATEST({Open}, {Close}) AS {Max_of_candle_body},\n",
    "                        LEAST({Open}, {Close}) AS {Min_of_candle_body},\n",
    "                        ABS({Open} - {Close}) AS {Body_height}\n",
    "                    FROM {data_stock}\n",
    "                    ORDER BY {Date}\n",
    "                ) AS {data_stock_sub_2}\n",
    "            ) AS {data_stock_sub_3}\n",
    "        ) AS {data_stock_sub_4}\n",
    "    )\n",
    "    ,\n",
    "    {data_stock_include_past_and_future_date} AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            LAG({Date_normalized}, 1, {Date_normalized}) OVER(ORDER BY {Date_normalized}) AS {Previous_date},\n",
    "            LAG({Close}, 1, 0) OVER(ORDER BY {Date_normalized}) AS {Close_of_previous_day},\n",
    "            LAG({Open}, 1, 0) OVER(ORDER BY {Date_normalized}) AS {Open_of_previous_day},\n",
    "            LAG({High}, 1, 0) OVER(ORDER BY {Date_normalized}) AS {High_of_previous_day},\n",
    "            LAG({Low}, 1, 0) OVER(ORDER BY {Date_normalized}) AS {Low_of_previous_day},\n",
    "            LAG({Is_candle_up}) OVER(ORDER BY {Date_normalized}) AS {Is_previous_day_candle_up},\n",
    "            LAG({Is_star_doji_candle}) OVER(ORDER BY {Date_normalized}) AS {Is_previous_day_start_doji_candle},\n",
    "            LAG({Body_height}) OVER(ORDER BY {Date_normalized}) AS {Body_height_of_previous_day},\n",
    "            LAG({Max_of_candle_body}) OVER(ORDER BY {Date_normalized}) AS {Max_of_candle_body_of_previous_day},\n",
    "            LAG({Min_of_candle_body}) OVER(ORDER BY {Date_normalized}) AS {Min_of_candle_body_of_previous_day},\n",
    "            \n",
    "            LEAD({Date_normalized}) OVER(ORDER BY {Date_normalized}) AS {Next_date},\n",
    "            LEAD({Close}) OVER(ORDER BY {Date_normalized}) AS {Close_of_next_day},\n",
    "            LEAD({Open}) OVER(ORDER BY {Date_normalized}) AS {Open_of_next_day},\n",
    "            LEAD({High}) OVER(ORDER BY {Date_normalized}) AS {High_of_next_day},\n",
    "            LEAD({Low}) OVER(ORDER BY {Date_normalized}) AS {Low_of_next_day},\n",
    "            LEAD({Is_candle_up}) OVER(ORDER BY {Date_normalized}) AS {Is_next_day_candle_up},\n",
    "            LEAD({Is_star_doji_candle}) OVER(ORDER BY {Date_normalized}) AS {Is_next_day_start_doji_candle},\n",
    "            LEAD({Body_height}) OVER(ORDER BY {Date_normalized}) AS {Body_height_of_next_day},\n",
    "            LEAD({Max_of_candle_body}) OVER(ORDER BY {Date_normalized}) AS {Max_of_candle_body_of_next_day},\n",
    "            LEAD({Min_of_candle_body}) OVER(ORDER BY {Date_normalized}) AS {Min_of_candle_body_of_next_day},\n",
    "\n",
    "            LEAD({Date_normalized}, 2) OVER(ORDER BY {Date_normalized}) AS {Next_2_date},\n",
    "            LEAD({Close}, 2) OVER(ORDER BY {Date_normalized}) AS {Close_of_next_2_day},\n",
    "            LEAD({Open}, 2) OVER(ORDER BY {Date_normalized}) AS {Open_of_next_2_day},\n",
    "            LEAD({High}, 2) OVER(ORDER BY {Date_normalized}) AS {High_of_next_2_day},\n",
    "            LEAD({Low}, 2) OVER(ORDER BY {Date_normalized}) AS {Low_of_next_2_day},\n",
    "            LEAD({Is_candle_up}, 2) OVER(ORDER BY {Date_normalized}) AS {Is_next_2_day_candle_up},\n",
    "            LEAD({Is_star_doji_candle}, 2) OVER(ORDER BY {Date_normalized}) AS {Is_next_2_day_start_doji_candle},\n",
    "            LEAD({Body_height}, 2) OVER(ORDER BY {Date_normalized}) AS {Body_height_of_next_2_day},\n",
    "            LEAD({Max_of_candle_body}, 2) OVER(ORDER BY {Date_normalized}) AS {Max_of_candle_body_of_next_2_day},\n",
    "            LEAD({Min_of_candle_body}, 2) OVER(ORDER BY {Date_normalized}) AS {Min_of_candle_body_of_next_2_day}\n",
    "        FROM {data_stock_sub_1}\n",
    "    )\n",
    "    ,\n",
    "    {data_stock_include_engulfing} AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            CASE\n",
    "                WHEN \n",
    "                    NOT {Is_candle_up} \n",
    "                    AND {Is_next_day_candle_up} \n",
    "                    AND {Max_of_candle_body} < {Max_of_candle_body_of_next_day} \n",
    "                    AND ABS({Min_of_candle_body} - {Min_of_candle_body_of_next_day}) / {Body_height_of_next_day} * 100  <= 10 \n",
    "                    AND {Body_height} / {Body_height_of_next_day} * 100 BETWEEN 30 AND 50\n",
    "                THEN True\n",
    "                ELSE False\n",
    "            END AS {Is_start_bullish_engulfing_candle},\n",
    "            CASE\n",
    "                WHEN\n",
    "                    {Is_candle_up} \n",
    "                    AND NOT {Is_next_day_candle_up} \n",
    "                    AND {Min_of_candle_body} > {Min_of_candle_body_of_next_day} \n",
    "                    AND ABS({Max_of_candle_body} - {Max_of_candle_body_of_next_day}) / {Body_height_of_next_day} * 100  <= 10 \n",
    "                    AND {Body_height} / {Body_height_of_next_day} * 100 BETWEEN 30 AND 50\n",
    "                THEN True\n",
    "                ELSE False\n",
    "            END AS {Is_start_bearish_engulfing_candle}\n",
    "        FROM {data_stock_include_past_and_future_date}\n",
    "    )\n",
    "    ,\n",
    "    {recent_engulfing_type} AS (\n",
    "        SELECT\n",
    "            {Date_normalized},\n",
    "            LAG({Is_start_bullish_engulfing_candle}, 1, {Is_start_bullish_engulfing_candle}) OVER(ORDER BY {Date_normalized}) AS {Is_recent_candle_is_bullish_engulfing},\n",
    "            LAG({Is_start_bearish_engulfing_candle}, 1, {Is_start_bearish_engulfing_candle}) OVER(ORDER BY {Date_normalized}) AS {Is_recent_candle_is_bearish_engulfing}\n",
    "        FROM {data_stock_include_engulfing}\n",
    "        WHERE {Is_start_bullish_engulfing_candle} OR {Is_start_bearish_engulfing_candle}\n",
    "    )\n",
    "    ,\n",
    "    {data_stock_include_group_engulfing} AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            MAX({Start_engulfing_candle_type}) OVER(PARTITION BY {Group_id_engulfing}) AS {Group_engulfing_type}\n",
    "        FROM (\n",
    "            SELECT\n",
    "                {data_stock_include_engulfing}.*,\n",
    "                SUM(\n",
    "                    CASE\n",
    "                        WHEN {Is_start_bullish_engulfing_candle} AND {Is_recent_candle_is_bearish_engulfing} THEN 1\n",
    "                        WHEN {Is_start_bearish_engulfing_candle} AND {Is_recent_candle_is_bullish_engulfing} THEN 1\n",
    "                        ELSE 0\n",
    "                    END\n",
    "                ) OVER(ORDER BY {data_stock_include_engulfing}.{Date_normalized}) AS {Group_id_engulfing},\n",
    "                CASE\n",
    "                    WHEN {Is_start_bullish_engulfing_candle} THEN \"{GROUP_BULL}\"\n",
    "                    WHEN {Is_start_bearish_engulfing_candle} THEN \"{GROUP_BEAR}\"\n",
    "                    ELSE \"\"\n",
    "                END AS {Start_engulfing_candle_type}\n",
    "            FROM {data_stock_include_engulfing} \n",
    "            LEFT JOIN {recent_engulfing_type}\n",
    "                ON {recent_engulfing_type}.{Date_normalized} = {data_stock_include_engulfing}.{Date_normalized}\n",
    "        ) AS {data_stock_include_group_engulfing_sub_1}\n",
    "    )\n",
    "    ,\n",
    "    {start_and_end_date_of_group_engulfing} AS (\n",
    "        SELECT\n",
    "            {Group_id_engulfing},\n",
    "            {Group_engulfing_type},\n",
    "            MIN({data_stock_include_group_engulfing}.{Date_normalized}) AS {Start_date_of_group},\n",
    "            MAX({data_stock_include_group_engulfing}.{Date_normalized}) AS {End_date_of_group},\n",
    "            COUNT(*) AS {Number_days_in_group}\n",
    "        FROM {data_stock_include_group_engulfing} \n",
    "        GROUP BY {Group_id_engulfing}, {Group_engulfing_type}\n",
    "    )\n",
    "    ,\n",
    "    {data_stock_include_harami} AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            CASE\n",
    "                WHEN \n",
    "                    NOT {Is_candle_up} \n",
    "                    AND {Is_next_day_candle_up} \n",
    "                    AND {Max_of_candle_body} > {Max_of_candle_body_of_next_day} \n",
    "                    AND ABS({Min_of_candle_body} - {Min_of_candle_body_of_next_day}) / {Body_height} * 100  <= 10 \n",
    "                    AND {Body_height} > {Body_height_of_next_day}\n",
    "                THEN True\n",
    "                ELSE False\n",
    "            END AS {Is_start_harami_up_candle},\n",
    "            CASE\n",
    "                WHEN\n",
    "                    {Is_candle_up} \n",
    "                    AND NOT {Is_next_day_candle_up} \n",
    "                    AND {Min_of_candle_body} < {Min_of_candle_body_of_next_day} \n",
    "                    AND ABS({Max_of_candle_body} - {Max_of_candle_body_of_next_day}) / {Body_height} * 100  <= 10 \n",
    "                    AND {Body_height} > {Body_height_of_next_day}\n",
    "                THEN True\n",
    "                ELSE False\n",
    "            END AS {Is_start_harami_down_candle}\n",
    "        FROM {data_stock_include_group_engulfing}\n",
    "    )\n",
    "    ,\n",
    "    {data_stock_include_tweezer} AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            CASE \n",
    "                WHEN \n",
    "                    {Is_candle_up}\n",
    "                    AND NOT {Is_next_day_candle_up}\n",
    "                    AND {Body_height} / {Body_height_of_next_day} * 100 BETWEEN 90 AND 110\n",
    "                THEN True\n",
    "                ELSE False\n",
    "            END AS {Is_start_tweezer_top_candle},\n",
    "            CASE \n",
    "                WHEN \n",
    "                    NOT {Is_candle_up}\n",
    "                    AND {Is_next_day_candle_up}\n",
    "                    AND {Body_height} / {Body_height_of_next_day} * 100 BETWEEN 90 AND 110\n",
    "                THEN True\n",
    "                ELSE False\n",
    "            END AS {Is_start_tweezer_bottom_candle}\n",
    "            \n",
    "        FROM {data_stock_include_harami}\n",
    "    )\n",
    "    ,\n",
    "    {data_stock_include_morning_and_evening_star} AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            CASE\n",
    "                WHEN\n",
    "                    NOT {Is_candle_up}\n",
    "                    AND {Is_next_2_day_candle_up}\n",
    "                    AND {Body_height_of_next_day} / {Body_height} * 100 <= 20\n",
    "                    AND {Body_height_of_next_day} / {Body_height_of_next_2_day} * 100 <= 20\n",
    "                THEN True\n",
    "                ELSE False\n",
    "            END AS {Is_start_morning_star_candle},\n",
    "            CASE\n",
    "                WHEN\n",
    "                    {Is_candle_up}\n",
    "                    AND NOT {Is_next_2_day_candle_up}\n",
    "                    AND {Body_height_of_next_day} / {Body_height} * 100 <= 20\n",
    "                    AND {Body_height_of_next_day} / {Body_height_of_next_2_day} * 100 <= 20\n",
    "                THEN True\n",
    "                ELSE False\n",
    "            END AS {Is_start_evening_star_candle}\n",
    "        FROM {data_stock_include_tweezer}\n",
    "    )\n",
    "    ,\n",
    "    {data_stock_include_next_3_day} AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            CASE\n",
    "                WHEN \n",
    "                    {Is_candle_up}\n",
    "                    AND {Is_next_day_candle_up}\n",
    "                    AND {Is_next_2_day_candle_up}\n",
    "                    AND NOT {Is_next_2_day_start_doji_candle}\n",
    "                THEN True\n",
    "                ELSE False\n",
    "            END AS {Is_start_next_3_day_up_candle},\n",
    "            CASE\n",
    "                WHEN \n",
    "                    NOT {Is_candle_up}\n",
    "                    AND NOT {Is_next_day_candle_up}\n",
    "                    AND NOT {Is_next_2_day_candle_up}\n",
    "                    AND NOT {Is_next_2_day_start_doji_candle}\n",
    "                THEN True\n",
    "                ELSE False\n",
    "            END AS {Is_start_next_3_day_down_candle}\n",
    "        FROM {data_stock_include_morning_and_evening_star}\n",
    "    )\n",
    "    ,\n",
    "    {recent_next_3_day_type} AS (\n",
    "        SELECT\n",
    "            {Date_normalized},\n",
    "            LAG({Is_start_next_3_day_up_candle}, 1, {Is_start_next_3_day_up_candle}) OVER(ORDER BY {Date_normalized}) AS {Is_recent_candle_is_next_3_day_up},\n",
    "            LAG({Is_start_next_3_day_down_candle}, 1, {Is_start_next_3_day_down_candle}) OVER(ORDER BY {Date_normalized}) AS {Is_recent_candle_is_next_3_day_down}\n",
    "        FROM {data_stock_include_next_3_day}\n",
    "        WHERE {Is_start_next_3_day_up_candle} OR {Is_start_next_3_day_down_candle}\n",
    "    )\n",
    "    ,\n",
    "    {data_stock_include_group_next_3_day} AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            MAX({Start_next_3_day_candle_type}) OVER(PARTITION BY {Group_id_next_3_day}) AS {Group_next_3_day_type},\n",
    "            ROW_NUMBER() OVER(PARTITION BY {Group_id_next_3_day} ORDER BY {Date_normalized}) AS {Index_in_group_next_3_day}\n",
    "        FROM (\n",
    "            SELECT\n",
    "                {data_stock_include_next_3_day}.*,\n",
    "                SUM(\n",
    "                    CASE\n",
    "                        WHEN {Is_start_next_3_day_up_candle} AND {Is_recent_candle_is_next_3_day_down} THEN 1\n",
    "                        WHEN {Is_start_next_3_day_down_candle} AND {Is_recent_candle_is_next_3_day_up} THEN 1\n",
    "                        ELSE 0\n",
    "                    END\n",
    "                ) OVER(ORDER BY {data_stock_include_next_3_day}.{Date_normalized}) AS {Group_id_next_3_day},\n",
    "                CASE\n",
    "                    WHEN {Is_start_next_3_day_up_candle} THEN \"{GROUP_BULL}\"\n",
    "                    WHEN {Is_start_next_3_day_down_candle} THEN \"{GROUP_BEAR}\"\n",
    "                    ELSE \"\"\n",
    "                END AS {Start_next_3_day_candle_type}\n",
    "            FROM {data_stock_include_next_3_day} \n",
    "            LEFT JOIN {recent_next_3_day_type}\n",
    "                ON {recent_next_3_day_type}.{Date_normalized} = {data_stock_include_next_3_day}.{Date_normalized}\n",
    "        ) AS {data_stock_include_group_next_3_day_sub_1}\n",
    "    )\n",
    "    ,\n",
    "    {last_candle_index_in_group_next_3_day} AS (\n",
    "        SELECT\n",
    "            {Group_id_next_3_day},\n",
    "            MAX({Max_index_in_group_next_3_day}) AS {Max_index_in_group_next_3_day}\n",
    "        FROM (\n",
    "            SELECT\n",
    "                *,\n",
    "                MAX({Index_in_group_next_3_day}) OVER (PARTITION BY {Group_id_next_3_day}) AS {Max_index_in_group_next_3_day}\n",
    "            FROM {data_stock_include_group_next_3_day}\n",
    "            WHERE {Is_start_next_3_day_up_candle} OR {Is_start_next_3_day_down_candle}\n",
    "        ) AS {last_candle_index_in_group_next_3_day_sub_1}\n",
    "        GROUP BY {Group_id_next_3_day}\n",
    "    )\n",
    "    ,\n",
    "    {data_stock_include_group_sideway} AS (\n",
    "        SELECT\n",
    "            a.*,\n",
    "            b.{Max_index_in_group_next_3_day},\n",
    "            CASE\n",
    "                WHEN a.{Index_in_group_next_3_day} <= b.{Max_index_in_group_next_3_day} + 2 THEN a.{Group_next_3_day_type}\n",
    "                ELSE \"{GROUP_SIZEWAY}\"\n",
    "            END AS {Group_trend_3_day_type}\n",
    "        FROM {data_stock_include_group_next_3_day} AS a\n",
    "        LEFT JOIN {last_candle_index_in_group_next_3_day} AS b\n",
    "            ON a.{Group_id_next_3_day} = b.{Group_id_next_3_day}\n",
    "    )\n",
    "    ,\n",
    "    {data_stock_base} AS (\n",
    "        SELECT\n",
    "            *\n",
    "        FROM {data_stock_include_group_sideway}\n",
    "    )\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_month_test_in_year(total_years, years_to_test):\n",
    "    month_test_in_year = dict()\n",
    "    for year in total_years:\n",
    "        if year in years_to_test:\n",
    "            number_month = years_to_test[year]\n",
    "            month_test_in_year[year] = []\n",
    "            for _ in range(number_month):\n",
    "                if year == 2014:\n",
    "                    month_test = random.randint(9, 12)\n",
    "                elif year == 2024:\n",
    "                    month_test = random.randint(1, 11)\n",
    "                else:\n",
    "                    month_test = random.randint(1, 12)\n",
    "                    \n",
    "                month_test_in_year[year].append(month_test)\n",
    "        else:\n",
    "            month_test_in_year[year] = [0]\n",
    "    return month_test_in_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_random_month_test_in_year(total_years, years_to_test):\n",
    "    month_test_in_year = dict()\n",
    "    for year in total_years:\n",
    "        if year in years_to_test:\n",
    "            number_month = years_to_test[year]\n",
    "            month_test_in_year[year] = years_to_test[year]\n",
    "        else:\n",
    "            month_test_in_year[year] = [0]\n",
    "    return month_test_in_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_test_in_year = not_random_month_test_in_year(\n",
    "    years, \n",
    "    years_to_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_test_in_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_exclude_month_query_for_train(year_and_months):\n",
    "    list_query = []\n",
    "    for year in year_and_months:\n",
    "        months = year_and_months[year]\n",
    "        list_query_of_year = []\n",
    "        for m in months:\n",
    "            q = f\"(YEAR({Date_normalized}) = {year} AND MONTH({Date_normalized}) != {m})\"\n",
    "            list_query_of_year.append(q)\n",
    "        query_of_year = \" AND \".join(list_query_of_year)\n",
    "        list_query.append(f\"({query_of_year})\")\n",
    "    return \" OR \".join(list_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_include_month_query_for_test(year_and_months):\n",
    "    list_query = []\n",
    "    for year in year_and_months:\n",
    "        months = year_and_months[year]\n",
    "        list_query_of_year = []\n",
    "        for m in months:\n",
    "            q = f\"(YEAR({Date_normalized}) = {year} AND MONTH({Date_normalized}) = {m})\"\n",
    "            list_query_of_year.append(q)\n",
    "        query_of_year = \" OR \".join(list_query_of_year)\n",
    "        list_query.append(f\"({query_of_year})\")\n",
    "    return \" OR \".join(list_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_exclude_month_query_for_train(month_test_in_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_include_month_query_for_test(month_test_in_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_image_of_input(\n",
    "    df_date: pd.DataFrame, \n",
    "    df_original: pd.DataFrame, \n",
    "    previous_days: int, \n",
    "    next_days: int,\n",
    "    days_result: int,\n",
    "    figscale: int,\n",
    "    number_images_preview = 0,\n",
    "    number_images_to_save = None,\n",
    "    indicator = {}\n",
    "    ):\n",
    "    \n",
    "    total_days = previous_days + 1 + next_days + days_result\n",
    "    if len(df_date) < total_days:\n",
    "        return\n",
    "    \n",
    "    count = 0\n",
    "    total_images = df_date[Total_records][0]\n",
    "    print(f\"Total: {total_images} images\")\n",
    "    \n",
    "    df_original = convert_number_index(df_original)\n",
    "    \n",
    "    list_images = []\n",
    "    list_targets = []\n",
    "    list_dates = []\n",
    "    \n",
    "    random_image_index_show = np.random.randint(0, total_images, size=number_images_preview)\n",
    "    \n",
    "    list_date_index_printed = []\n",
    "    not_save_image_count = 0\n",
    "    \n",
    "    for date in df_date[Date_normalized]:\n",
    "        date_index = get_date_index(df_original, date)\n",
    "        \n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Load {count}/{total_images} images\")\n",
    "            \n",
    "        if len(list_date_index_printed) > 0:\n",
    "            last_index_checked = list_date_index_printed[-1]\n",
    "            if date_index - last_index_checked <= next_days:\n",
    "                not_save_image_count += 1\n",
    "                continue\n",
    "\n",
    "        list_date_index_printed.append(date_index)\n",
    "            \n",
    "        if number_images_to_save is not None and count > number_images_to_save:\n",
    "            break\n",
    "        \n",
    "        draw_df_pandas = add_days_around_date(date, df_original, previous_days, next_days)\n",
    "        if len(draw_df_pandas) < (previous_days + next_days):\n",
    "            continue\n",
    "        \n",
    "        is_preview_image = count in random_image_index_show\n",
    "        image_tensor = draw_candle_image(\n",
    "            draw_df_pandas, \n",
    "            show_x_y=False, \n",
    "            show_volume=False, \n",
    "            \n",
    "            **indicator,\n",
    "            \n",
    "            figscale=figscale, \n",
    "            figcolor=\"black\", \n",
    "            preview_image=is_preview_image,\n",
    "            return_image_tensor=True\n",
    "        )\n",
    "        \n",
    "        image_tensor = normalize_candle_image(\n",
    "            image_tensor, \n",
    "            combine_into_one_image=False,\n",
    "            convert_red=False,\n",
    "            convert_green=False,\n",
    "        )\n",
    "        \n",
    "        date = normalize_date(date)\n",
    "        index_of_date = get_date_index(df_original, date)\n",
    "        start_index = index_of_date - previous_days\n",
    "        end_index = index_of_date + next_days + days_result\n",
    "        data_result_df = df_original[start_index:end_index + 1]\n",
    "    \n",
    "        if 0 < len(data_result_df) < total_days:\n",
    "            data_result_df = duplicate_last_row(data_result_df, total_days - len(data_result_df))\n",
    "        elif len(data_result_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        targets = data_result_df[[High, Open, Close, Low]].to_numpy()\n",
    "        \n",
    "        list_images.append(image_tensor)\n",
    "        list_targets.append(targets)\n",
    "        list_dates.append(str(date))\n",
    "    \n",
    "    return list_images, list_targets, list_dates, not_save_image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_two_image_of_input(\n",
    "    df_date: pd.DataFrame, \n",
    "    df_original: pd.DataFrame, \n",
    "    previous_days: int, \n",
    "    next_days: int,\n",
    "    days_result: int,\n",
    "    figscale: int,\n",
    "    number_images_preview = 0,\n",
    "    number_images_to_save = None,\n",
    "    indicator = {}\n",
    "    ):\n",
    "    \n",
    "    total_days = previous_days + 1 + next_days + days_result\n",
    "    if len(df_date) < total_days:\n",
    "        return\n",
    "    \n",
    "    count = 0\n",
    "    total_images = df_date[Total_records][0]\n",
    "    print(f\"Total: {total_images} images\")\n",
    "    \n",
    "    df_original = convert_number_index(df_original)\n",
    "    \n",
    "    list_images = []\n",
    "    list_targets = []\n",
    "    list_dates = []\n",
    "    \n",
    "    random_image_index_show = np.random.randint(0, total_images, size=number_images_preview)\n",
    "    \n",
    "    list_date_index_printed = []\n",
    "    not_save_image_count = 0\n",
    "    \n",
    "    for date in df_date[Date_normalized]:\n",
    "        date_index = get_date_index(df_original, date)\n",
    "        \n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Load {count}/{total_images} images\")\n",
    "            \n",
    "        if len(list_date_index_printed) > 0:\n",
    "            last_index_checked = list_date_index_printed[-1]\n",
    "            if date_index - last_index_checked <= next_days:\n",
    "                not_save_image_count += 1\n",
    "                continue\n",
    "\n",
    "        list_date_index_printed.append(date_index)\n",
    "            \n",
    "        if number_images_to_save is not None and count > number_images_to_save:\n",
    "            break\n",
    "        \n",
    "        draw_df_pandas = add_days_around_date(date, df_original, previous_days, next_days)\n",
    "        if len(draw_df_pandas) < (previous_days + next_days):\n",
    "            continue\n",
    "        \n",
    "        is_preview_image = count in random_image_index_show\n",
    "        image_tensor = draw_candle_image(\n",
    "            draw_df_pandas, \n",
    "            show_x_y=False, \n",
    "            show_volume=False, \n",
    "            \n",
    "            **indicator,\n",
    "            \n",
    "            figscale=figscale, \n",
    "            figcolor=\"black\", \n",
    "            preview_image=is_preview_image,\n",
    "            return_image_tensor=True\n",
    "        )\n",
    "        \n",
    "        image_tensor = normalize_candle_image(\n",
    "            image_tensor, \n",
    "            combine_into_one_image=False,\n",
    "            convert_red=False,\n",
    "            convert_green=False,\n",
    "        )\n",
    "        \n",
    "        date = normalize_date(date)\n",
    "        index_of_date = get_date_index(df_original, date)\n",
    "        start_index = index_of_date - previous_days\n",
    "        end_index = index_of_date + next_days + days_result\n",
    "        data_result_df = df_original[start_index:end_index + 1]\n",
    "    \n",
    "        if 0 < len(data_result_df) < total_days:\n",
    "            data_result_df = duplicate_last_row(data_result_df, total_days - len(data_result_df))\n",
    "        elif len(data_result_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        targets = data_result_df[[High, Open, Close, Low]].to_numpy()\n",
    "        \n",
    "        list_images.append(image_tensor)\n",
    "        list_targets.append(targets)\n",
    "        list_dates.append(str(date))\n",
    "    \n",
    "    return list_images, list_targets, list_dates, not_save_image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_tensorflow_dataset(\n",
    "    df_date: pd.DataFrame, \n",
    "    df_original: pd.DataFrame, \n",
    "    previous_days: int, \n",
    "    next_days: int,\n",
    "    days_result: int,\n",
    "    figscale: int,\n",
    "    number_images_preview = 0,\n",
    "    save_dataset_to_folder = None,\n",
    "    number_images_to_save = None,\n",
    "    indicator = {},\n",
    "    function_generate = None,\n",
    "    ):\n",
    "    \n",
    "    list_images, list_targets, list_dates, not_print_image_count = function_generate(\n",
    "        df_date,\n",
    "        df_original,\n",
    "        previous_days,\n",
    "        next_days,\n",
    "        days_result,\n",
    "        figscale,\n",
    "        number_images_preview,\n",
    "        number_images_to_save,\n",
    "        indicator,\n",
    "    )\n",
    "    \n",
    "            \n",
    "    list_images = np.array(list_images)\n",
    "    list_targets = np.array(list_targets)\n",
    "    list_dates = np.array(list_dates)\n",
    "    \n",
    "    print(f\"Not print date: {not_print_image_count} images\")\n",
    "    print(f\"list_images shape = {list_images.shape}\")\n",
    "    print(f\"list_targets shape = {list_targets.shape}\")\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list_images, list_targets, list_dates))\n",
    "    \n",
    "    if save_dataset_to_folder is not None:\n",
    "        if not os.path.exists(save_dataset_to_folder):\n",
    "            os.makedirs(save_dataset_to_folder)\n",
    "        else:\n",
    "            exist_ok = input(f\"Folder {save_dataset_to_folder} already exists, continue? [y/n]: \").lower()[0] == \"y\"\n",
    "            if not exist_ok:\n",
    "                return dataset\n",
    "    \n",
    "        tf.data.Dataset.save(dataset, save_dataset_to_folder)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_trend_type = {\n",
    "    GROUP_BULL: 1,\n",
    "    GROUP_BEAR: -1,\n",
    "    GROUP_SIZEWAY: 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_image_of_input_with_trend_type(\n",
    "    df_date: pd.DataFrame, \n",
    "    df_original: pd.DataFrame, \n",
    "    previous_days: int, \n",
    "    next_days: int,\n",
    "    days_result: int,\n",
    "    figscale: int,\n",
    "    number_images_preview = 0,\n",
    "    number_images_to_save = None,\n",
    "    indicator = {}\n",
    "    ):\n",
    "    \n",
    "    list_trend_type_values = []\n",
    "    list_images = []\n",
    "    list_targets = []\n",
    "    list_dates = []\n",
    "    \n",
    "    if len(df_date) == 0:\n",
    "        print(f\"Total: 0 images\")\n",
    "        return list_trend_type_values, list_images, list_targets, list_dates, 0\n",
    "    \n",
    "    total_days = previous_days + 1 + next_days + days_result\n",
    "    \n",
    "    count = 0\n",
    "    total_images = df_date[Total_records][0]\n",
    "    print(f\"Total: {total_images} images\")\n",
    "    \n",
    "    df_original = convert_number_index(df_original)\n",
    "    \n",
    "    random_image_index_show = np.random.randint(0, total_images, size=number_images_preview)\n",
    "    \n",
    "    list_date_index_printed = []\n",
    "    not_save_image_count = 0\n",
    "    \n",
    "    for date in df_date[Date_normalized]:\n",
    "        date_str = date\n",
    "        date_index = get_date_index(df_original, date)\n",
    "        \n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Load {count}/{total_images} images\")\n",
    "            \n",
    "        if len(list_date_index_printed) > 0:\n",
    "            last_index_checked = list_date_index_printed[-1]\n",
    "            if date_index - last_index_checked <= next_days:\n",
    "                not_save_image_count += 1\n",
    "                continue\n",
    "\n",
    "        list_date_index_printed.append(date_index)\n",
    "            \n",
    "        if number_images_to_save is not None and count > number_images_to_save:\n",
    "            break\n",
    "        \n",
    "        draw_df_pandas = add_days_around_date(date, df_original, previous_days, next_days)\n",
    "        if len(draw_df_pandas) < (previous_days + next_days):\n",
    "            continue\n",
    "        \n",
    "        is_preview_image = count in random_image_index_show\n",
    "        image_tensor = draw_candle_image(\n",
    "            draw_df_pandas, \n",
    "            show_x_y=False, \n",
    "            show_volume=False, \n",
    "            \n",
    "            **indicator,\n",
    "            \n",
    "            figscale=figscale, \n",
    "            figcolor=\"black\", \n",
    "            preview_image=is_preview_image,\n",
    "            return_image_tensor=True\n",
    "        )\n",
    "        \n",
    "        image_tensor = normalize_candle_image(\n",
    "            image_tensor, \n",
    "            combine_into_one_image=False,\n",
    "            convert_red=False,\n",
    "            convert_green=False,\n",
    "        )\n",
    "        \n",
    "        date = normalize_date(date)\n",
    "        index_of_date = get_date_index(df_original, date)\n",
    "        start_index = index_of_date - previous_days\n",
    "        end_index = index_of_date + next_days + days_result\n",
    "        data_result_df = df_original[start_index:end_index + 1]\n",
    "    \n",
    "        if 0 < len(data_result_df) < total_days:\n",
    "            data_result_df = duplicate_last_row(data_result_df, total_days - len(data_result_df))\n",
    "        elif len(data_result_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        targets = data_result_df[[High, Open, Close, Low]].to_numpy()\n",
    "        \n",
    "        trend_type_df = df_date[df_date[Date_normalized] == date_str][[Group_trend_3_day_type]]\n",
    "        trend_type = trend_type_df.values[0][0]\n",
    "        \n",
    "        list_images.append(image_tensor)\n",
    "        list_targets.append(targets)\n",
    "        list_dates.append(str(date))\n",
    "        list_trend_type_values.append(mapping_trend_type[str(trend_type)])\n",
    "    \n",
    "    return list_trend_type_values, list_images, list_targets, list_dates, not_save_image_count\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_tensorflow_dataset_with_trend_type(\n",
    "    df_date: pd.DataFrame, \n",
    "    df_original: pd.DataFrame, \n",
    "    previous_days: int, \n",
    "    next_days: int,\n",
    "    days_result: int,\n",
    "    figscale: int,\n",
    "    number_images_preview = 0,\n",
    "    save_dataset_to_folder = None,\n",
    "    number_images_to_save = None,\n",
    "    indicator = {},\n",
    "    function_generate = None,\n",
    "    ):\n",
    "    \n",
    "    list_trend_type_values, list_images, list_targets, list_dates, not_print_image_count = function_generate(\n",
    "        df_date,\n",
    "        df_original,\n",
    "        previous_days,\n",
    "        next_days,\n",
    "        days_result,\n",
    "        figscale,\n",
    "        number_images_preview,\n",
    "        number_images_to_save,\n",
    "        indicator,\n",
    "    )\n",
    "    \n",
    "            \n",
    "    list_images = np.array(list_images)\n",
    "    list_targets = np.array(list_targets)\n",
    "    list_dates = np.array(list_dates)\n",
    "    list_trend_type_values = np.array(list_trend_type_values).reshape(-1, 1)\n",
    "    \n",
    "    print(f\"Not print date: {not_print_image_count} images\")\n",
    "    print(f\"list_images shape = {list_images.shape}\")\n",
    "    print(f\"list_targets shape = {list_targets.shape}\")\n",
    "    print(f\"list_trend_type_values shape = {list_trend_type_values.shape}\")\n",
    "    \n",
    "    if len(list_images) == 0:\n",
    "        return None\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list_trend_type_values, list_images, list_targets, list_dates))\n",
    "    \n",
    "    if save_dataset_to_folder is not None:\n",
    "        if not os.path.exists(save_dataset_to_folder):\n",
    "            os.makedirs(save_dataset_to_folder)\n",
    "        else:\n",
    "            exist_ok = input(f\"Folder {save_dataset_to_folder} already exists, continue? [y/n]: \").lower()[0] == \"y\"\n",
    "            if not exist_ok:\n",
    "                return dataset\n",
    "    \n",
    "        tf.data.Dataset.save(dataset, save_dataset_to_folder)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = f\"\"\"\n",
    "#     {base_query}\n",
    "            \n",
    "#     SELECT\n",
    "#         {Group_trend_3_day_type},\n",
    "#         {Date_normalized},\n",
    "        \n",
    "#         {Is_start_next_3_day_up_candle},\n",
    "#         {Is_start_morning_star_candle},\n",
    "#         {Is_start_harami_up_candle},\n",
    "#         {Is_start_bullish_engulfing_candle},\n",
    "#         {Is_start_tweezer_top_candle},\n",
    "        \n",
    "#         {Is_start_next_3_day_down_candle},\n",
    "#         {Is_start_evening_star_candle},\n",
    "#         {Is_start_harami_down_candle},\n",
    "#         {Is_start_bearish_engulfing_candle},\n",
    "#         {Is_start_tweezer_bottom_candle},\n",
    "        \n",
    "        \n",
    "#         {Is_star_doji_candle},\n",
    "#         {Is_dragonfly_doji_candle},\n",
    "#         {Is_gravestone_doji_candle},\n",
    "#         {Is_hammer_candle},\n",
    "#         {Is_inverted_hammer_candle},\n",
    "#         {Is_marubozu_candle},\n",
    "#         {Is_spin_candle},\n",
    "#         {Open},\n",
    "#         {Close},\n",
    "#         {High},\n",
    "#         {Low},\n",
    "#         {Body_height},\n",
    "#         {Date_normalized},\n",
    "#         *\n",
    "#     FROM {data_stock_base}\n",
    "#     ORDER BY {Date_normalized}\n",
    "# \"\"\"\n",
    "\n",
    "# df_pandas = spark.sql(query).toPandas()\n",
    "# df_pandas.to_excel(f\"./dump_df/btc_all_data_4.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candle_type = [\n",
    "    Is_start_bullish_engulfing_candle,\n",
    "    Is_start_bearish_engulfing_candle,\n",
    "    Is_star_doji_candle,\n",
    "    Is_dragonfly_doji_candle,\n",
    "    Is_gravestone_doji_candle,\n",
    "    Is_start_morning_star_candle,\n",
    "    Is_start_evening_star_candle,\n",
    "    Is_hammer_candle,\n",
    "    Is_inverted_hammer_candle,\n",
    "    Is_start_harami_down_candle,\n",
    "    Is_start_harami_up_candle,\n",
    "    Is_start_tweezer_top_candle,\n",
    "    Is_start_tweezer_bottom_candle,\n",
    "    Is_marubozu_candle,\n",
    "    Is_spin_candle,\n",
    "    Is_start_next_3_day_up_candle,\n",
    "    Is_start_next_3_day_down_candle\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candle_type_and_directory_save = {\n",
    "    Is_start_bullish_engulfing_candle: \"bullish_engulfing\",\n",
    "    Is_start_bearish_engulfing_candle: \"bearish_engulfing\",\n",
    "    Is_star_doji_candle: \"star_doji\",\n",
    "    Is_dragonfly_doji_candle: \"dragonfly_doji\",\n",
    "    Is_gravestone_doji_candle: \"gravestone_doji\",\n",
    "    Is_start_morning_star_candle: \"morning_star\",\n",
    "    Is_start_evening_star_candle: \"evening_star\",\n",
    "    Is_hammer_candle: \"hammer\",\n",
    "    Is_inverted_hammer_candle: \"inverted_hammer\",\n",
    "    Is_start_harami_down_candle: \"harami_down\",\n",
    "    Is_start_harami_up_candle: \"harami_up\",\n",
    "    Is_start_tweezer_top_candle: \"tweezer_top\",\n",
    "    Is_start_tweezer_bottom_candle: \"tweezer_bottom\",\n",
    "    Is_marubozu_candle: \"marubozu\",\n",
    "    Is_spin_candle: \"spin\",\n",
    "    Is_start_next_3_day_up_candle: \"next_3_day_up\",\n",
    "    Is_start_next_3_day_down_candle: \"next_3_day_down\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = f\"\"\"\n",
    "#     {base_query}\n",
    "    \n",
    "#     SELECT\n",
    "#         COUNT(*) OVER() AS {Total_records},\n",
    "#         *\n",
    "#     FROM {data_stock_base}\n",
    "#     WHERE {Is_gravestone_doji_candle}\n",
    "#     AND ({generate_include_month_query_for_test(month_test_in_year)})\n",
    "#     ORDER BY {Date_normalized}\n",
    "# \"\"\"\n",
    "\n",
    "# df_spark = spark.sql(query)\n",
    "# df_spark.show(2)\n",
    "# dataset = save_to_tensorflow_dataset_with_trend_type(\n",
    "#     df_date=df_spark.toPandas(),\n",
    "#     df_original=stock_df,\n",
    "#     previous_days=previous_days,\n",
    "#     next_days=next_days,\n",
    "#     days_result=days_result,\n",
    "#     number_images_preview=0,\n",
    "#     # save_dataset_to_folder=f\"{folder_save}/test/{directory_save}\",\n",
    "#     figscale=figscale,\n",
    "#     indicator=indicator,\n",
    "#     function_generate=generate_one_image_of_input_with_trend_type\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test\n",
    "is_run_for_test = input(\"Run for test? (y/n): \").lower()\n",
    "total_records_test = 0\n",
    "if is_run_for_test[0] == \"y\":\n",
    "    for candle_type, directory_save in candle_type_and_directory_save.items():\n",
    "        print(f\"{candle_type=}\")\n",
    "        print(f\"{directory_save=}\")\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            {base_query}\n",
    "            \n",
    "            SELECT\n",
    "                COUNT(*) OVER() AS {Total_records},\n",
    "                *\n",
    "            FROM {data_stock_base}\n",
    "            WHERE {candle_type}\n",
    "            AND ({generate_include_month_query_for_test(month_test_in_year)})\n",
    "            ORDER BY {Date_normalized}\n",
    "        \"\"\"\n",
    "        \n",
    "        df_spark = spark.sql(query)\n",
    "        df_spark.show(2)\n",
    "        dataset = save_to_tensorflow_dataset_with_trend_type(\n",
    "            df_date=df_spark.toPandas(),\n",
    "            df_original=stock_df,\n",
    "            previous_days=previous_days,\n",
    "            next_days=next_days,\n",
    "            days_result=days_result,\n",
    "            number_images_preview=0,\n",
    "            save_dataset_to_folder=f\"{folder_save}/test/{directory_save}\",\n",
    "            figscale=figscale,\n",
    "            indicator=indicator,\n",
    "            function_generate=generate_one_image_of_input_with_trend_type\n",
    "        )\n",
    "        \n",
    "        if dataset is None:\n",
    "            dataset = []\n",
    "        total_records_test += len(dataset)\n",
    "print(f\"Total records for test: {total_records_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train\n",
    "is_run_for_train = input(\"Run for train? (y/n): \").lower()\n",
    "total_records_train = 0\n",
    "if is_run_for_train[0] == \"y\":\n",
    "    for candle_type, directory_save in candle_type_and_directory_save.items():\n",
    "        print(f\"{candle_type=}\")\n",
    "        print(f\"{directory_save=}\")\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            {base_query}\n",
    "            \n",
    "            SELECT\n",
    "                COUNT(*) OVER() AS {Total_records},\n",
    "                *\n",
    "            FROM {data_stock_base}\n",
    "            WHERE {candle_type}\n",
    "            AND ({generate_exclude_month_query_for_train(month_test_in_year)})\n",
    "            ORDER BY {Date_normalized}\n",
    "        \"\"\"\n",
    "        \n",
    "        df_spark = spark.sql(query)\n",
    "        df_spark.show(2)\n",
    "        dataset = save_to_tensorflow_dataset_with_trend_type(\n",
    "            df_date=df_spark.toPandas(),\n",
    "            df_original=stock_df,\n",
    "            previous_days=previous_days,\n",
    "            next_days=next_days,\n",
    "            days_result=days_result,\n",
    "            number_images_preview=0,\n",
    "            save_dataset_to_folder=f\"{folder_save}/train/{directory_save}\",\n",
    "            figscale=figscale,\n",
    "            indicator=indicator,\n",
    "            function_generate=generate_one_image_of_input_with_trend_type\n",
    "        )\n",
    "        if dataset is None:\n",
    "            dataset = []\n",
    "        total_records_train += len(dataset)\n",
    "print(f\"Total records for train: {total_records_train}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_test_in_year_copy = month_test_in_year\n",
    "# for key, value in month_test_in_year.items():\n",
    "#     month_test_in_year_copy[int(key)] = int(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"month_test_in_year\": month_test_in_year_copy,\n",
    "    \"candle_type_and_directory_save\": candle_type_and_directory_save,\n",
    "    \"folder_save\": folder_save,\n",
    "    \"figscale\": figscale,\n",
    "    \"previous_days\": previous_days,\n",
    "    \"next_days\": next_days,\n",
    "    \"days_result\": days_result,\n",
    "    \"file_csv\": file_csv,\n",
    "    \n",
    "    \"indicator\": indicator,\n",
    "    \"total_records_train\": total_records_train,\n",
    "    \"total_records_test\": total_records_test,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{folder_save}/config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
